Select @@Version

/*

Purpose of SP2:
Service Packs that are released by Microsoft contain important fixes for the product and are vigorously tested before being released to the public; so they’re pretty important to install. But, as always, I highly recommend you apply them in a test server and test them thoroughly before applying to the production server.
--Determine the version and service pack of SQL Server
select @@version
http://www.microsoft.com/en-us/download/details.aspx?id=43340

All SQLServer service packs are cumulative, meaning that each new service pack contains all the fixes that are included with previous service packs and any new fixes.

-- CLASS 17 - INTRODUCTION TO SYSTEM DATABASES
SQL Server System Databases
---------------------------------------------------------------------------------------
Each time you install any SQL Server Edition on a server; there are four primary system databases, each of which must be present for the server to operate effectively. 

Master
	-- file locations of the user databases
    -- login accounts
    -- server configuration settings
    -- linked servers information
    -- startup stored procedures  

Model
    -- A template database that is copied into a new database 
    -- Options set in model will be applied to new databases
    -- Used to create tempdb every time the server starts  

Msdb
   -- Support SQL Server Agent
   -- SQL Server Management Studio
   -- Database Mail
   -- Service Broker which is messaging system
   -- History and metadata information is available in msdb
   -- Backup and restore history for the databases
   -- History for SQL agent jobs

Tempdb
   -- The tempdb is a shared resource used by SQL Server all users 
   -- Tempdb is used for temporary objects, worktables, online index operations, cursors, table variables, and the snapshot isolation version store, among other things
   -- It is recreated every time that the server is restarted
   -- As tempdb is non-permanent storage, backups and restores are not allowed for this database.  

Reporting Services Databases
   -- ReportServer - available if you have installed Reporting Services 
   -- ReportServerTempDB - available if you have installed Reporting Services

Replication System Database
   -- Distribution - only available when you configure Replication

Resource Database
   -- Read-only hidden database that contains all the system information
   -- Can't back up the Resource database
   -- Must copy paste the file
   -- C:\Program Files\Microsoft SQL Server\MSSQL11.MSSQLSERVER\MSSQL\Binn
*/


-- CLASS 18 - SQL Server Data Log Files
----------------------------------------------------------------------------------------
-- What is a database file?
   -- When you create a database, two primary files are created by default: the data file and the transaction log file.  The primary purpose of the data file is to hold all the data, such as tables, indexes, store procedures and other relevant data and the data file is simple to understand and requires some management, the transaction log file requires a greater attention and understanding.

-- What are a transaction log file and its purpose?
   -- The primary function of the transaction log file is to:
      -- 1. Record all changes to the database so anytime you modify table or delete any records it recorded it in transaction log
      -- 2. Record changes sequentially its good practice to have Transaction Log file in is own folder
      -- 3. All data is written to the transaction log file first before committing the changes to the data file
      -- 4. A triggering mechanism, called a checkpoint,  is triggered each few minutes to the transaction log to indicate that a particular transaction has been completely written from the buffer to the data file;  this process keeps flushes out the committed transaction, and maintains the size of the physical transaction log file (only in simple recovery mode) 
      -- 5. Key object needed to restore databases

      -- 6. Controls the size of the transaction log file and prevents the log consuming the disk space
      -- 7. Used in log shipping, database mirroring, and replication
      -- 8. Allows recover to a point in time

-- Reason the transaction log file is out of control in size
   -- Transaction log backups are not occurring while in Simple recovery mode
   -- Very long transactions are occurring, like indexes of table or many updates
   -- Demo to control the size of transaction log by doing log backups 

-- Stop
-- What are the recovery models and their roles?
   -- The recovery models in SQL Server, Simple, Full, Bulk-Logged, determine whether a transaction log backup is available or not

-- Simple recovery model
   -- Transaction log backups are not supported
   -- Truncation of the transaction log is done automatically, thereby releasing space to the system

-- You can lose data, as there are no transaction log backups
   -- When in recovery mode, data in the T-Log will not grow

-- Bulk-logged recovery model
   -- Supports transaction log backups 

-- As in Full mode, there is no automated process of transaction log truncation
   -- Used primarily for bulk operations, such as bulk insert, thereby minimal 

-- Full recovery model
   -- Supports transaction log backups 
   -- Little chance of data loss under the normal circumstances
   -- No automated process to truncate the log thus must have T-log backups

-- The transaction log backups must be made regularly to mark unused space available for overwriting

-- When using Full recovery mode, the T-log can get large in size 

-- During most backup processes, the changes contained in the log file are sent to the backup file



-- SHOW ALL THE USER DATABASES AND THEIR MODE 
-- Scripts for Recovery Models: 
select [name], DATABASEPROPERTYEX([name],'recovery')
from sysdatabases
where name not in ('master','model','tempdb','msdb')

-- SAME AS ABOVE QUERY
-- SHOW ALL THE USER DATABASES AND THEIR MODE 
Select name, recovery_model_desc
From Sys.databases
Where name not in ('master','model','tempdb','msdb')
Order by name 

-- Change the recovery mode:
USE master;
GO
-- Set recovery model to SIMPLE
ALTER DATABASE DBAdmin SET RECOVERY SIMPLE;
GO
-- Set recovery model to FULL
ALTER DATABASE DBAdmin SET RECOVERY FULL;
GO

Use master
Create Database HowTlogDB


-- CHECK THE USER DATABASES FOR THEIR MODE OF REVOVERY THEY ARE
Select name, recovery_model_desc
From Sys.databases
Where name not in ('master','model','tempdb','msdb')
Order by name 

-- CREATE TABLE
Use HowTlogDB
Create Table Products
(ProductID int Identity(1,1) Primary key,
ProductName varchar(50),
Brand varchar(50))

-- INSERT DATA INTO TABLE
Use HowTlogDB
Insert into Products Values ('Bread', 'Eat')
Insert into Products Values ('Mat', 'Nike')
Insert into Products Values ('Shoe', 'Lvt')
Insert into Products Values ('Phone', 'Apple')
Insert into Products Values ('Cup', 'Large')
Insert into Products Values ('Tv', 'Samsung')

-- VIEW DATA
Select * from Products

-- TAKE FULL DATABASE BACKUP OF SIX ROWS
Backup Database [HowTlogDB]
To Disk = N'I:\MSSQL\HowTlogDB_Full.bak'


-- SCRIPTS TO CHECK THE TRANSACTION LOG
   -- Demo that taking a full backup does not truncate the log file, but taking a transaction log file does truncate the log file

-- Drop the current database

EXEC msdb.dbo.sp_delete_database_backuphistory @database_name = N'ADMINDB'
GO

USE [master]
GO

ALTER DATABASE [AdminDB] SET  SINGLE_USER WITH ROLLBACK IMMEDIATE
GO

USE [master]
DROP DATABASE [AdminDB]
GO

-- Create a database for testing

  USE MASTER
  GO
  
  CREATE DATABASE AdminDB
  GO


-- Create a table and insert data from AdventureWorks2016.HumanResources.Employee

USE ADMINDB
GO

SELECT * 
INTO dbo.AAA
FROM AdventureWorks2016.HumanResources.Employee

SELECT * FROM AAA

-- Change the recovery mode to full so as to take transactional log backups

USE MASTER;
ALTER DATABASE ADMINDB
SET RECOVERY FULL;



-- View the space used and allocated to transaction log

DBCC SQLPERF (LOGSPACE)


-- Database Name	Log Size (MB)	Log Space Used (%)	Status
-- AdminDB	          7.992188	       5.681818           0

-- Take a full backup of database
Use AdminDB
BACKUP DATABASE AdminDB
TO DISK = 'I:\MSSQL\AdminDB_Full.bak';

-- Modify the database by updates, and deletes
SELECT * FROM AAA


USE AdminDB
GO

UPDATE AAA
SET MaritalStatus = 'S'
WHERE JobTitle = 'Design Engineer';

SELECT * FROM AAA

DELETE AAA
WHERE BusinessEntityID > 5; -- 1952-09-27
---------------------------------------------------------

SELECT * FROM AAA

-- Take a full database backup to set it in full mode
Use AdminDB
BACKUP DATABASE AdminDB
TO DISK = 'I:\MSSQL\AdminDB_Full.bak';


-- Check the space used by log file after full database backup, notice the log space used had not reduced in size!

DBCC SQLPERF (LOGSPACE);

-- Database Name	  Log Size (MB)	    Log Space Used (%)	   Status
--     AdminDB	         7.992188	       7.801808              0	 
-- OLD AdminDB	         7.992188	       5.681818              0

-- USE GUI => Take a transaction log backup.  Note that the size of the log file space used is reduced, but not the actual size of the file
-- This is because, when you take a log backup, the inactive transactions are removed from the log file to the backup file!

BACKUP log  AdminDB
TO DISK = 'F:\MSSQL\AdminDB_log.trn'

DBCC SQLPERF (LOGSPACE);

-- Database Name	Log Size (MB)	Log Space Used (%)	Status
-- ADMIN	          0.8046875	       7.862903           0


-- Viewing inside the SQL Server Database Transaction Log to prevent internal fragmentation
-- (Auto growth and sizing of auto growth log)
     -- There is a function called (fn_dblog) which requires a beginning LSN and an ending LSN (Log Sequence Number) for a transaction, but for our purpose we will use NULL as starting and ending values. The following example will show how the transaction log records multiple rows of data for any activity against the database and how a transaction log backup truncates the transaction file, once it’s written to the backup log.  (fn_dblog)  A ways to view the contents of the log.  DONOT RUN THIS COMMAND IN PRODUCTION AS IT IS UNDOCUMENTED.


-- CLASS - 19 => Details Transaction Log ATM Exampls
------------------------------------------------------------------------------------------------------------------
-- mdf -- Master Data File => Records the single transaction, records the amount on your account and if you take - 100 it record it and show balance of 400
-- ldf -- Log Data File => Records multiple things, records every steps happening in the ATM card Time inserting the card, Pin enter, withdraw money trasaction and  so on... and this why the files is getting bigger bcos it record multiple things and as more records keep coming LDFile keep grow by the auto growth until the physical size of the file is consumed which is very dangerous
  -- How to manage LDF files in other not be full
  -- How do will clear it or truncate it


-- Options to truncate the log file:
   -- 1.	Backup the database.
   -- 2.	Detach the database, 
   -- 3.	Delete the transaction log file. (or rename the file, just in case)
   -- 4.	Re-attach the database 
   -- 5.	Shrink the database
   -- 6.	None of the above


   -- CLASS - 20 => Auto Growth and Sizing of Transaction Log
   -------------------------------------------------------------------------------------------------------------------

 
-- Previous videos
-- • What is a transaction log file  -- file to record all changes to the database
-- • Why backup transaction log file -- to clear the transaction log and control size
-- • Inside the transaction log file -- Use DBCC LogInfo
-- • Out of control transaction log file -- primary reason, full recovery mode and no transactional backups


-- What is the auto growth feature?
   -- An auto-growth event is a part of SQL Server that expands the size of a database file when it runs out of space.  If there is a transaction (such as many inserts) that requires more log space than is available, the transaction log file of that database will need to adjust to the new space needed by increasing the log file size.  

-- What happens when auto growth is expanding?
   -- This can cause a performance issue, as this is a blocking operation. The transaction that initiated the log growth will be held until more space is allocated to the log file, determined by the auto growth setting

-- Physical fragmented on the disk occurs as the pages required are not necessarily next to each other. The more auto-growth events you have the more physical fragmentation you will have the files

-- Avoid auto growth by pro actively configuring the auto growth
   -- Pre-size the data and log files
   -- Manually manage the growth of data and log files 
   -- Auto growth should be used for safety reasons only

-- Don’t rely on auto growth
   -- Maintain a level of at least 25 percent available space across disks to allow for growth and peak usage patterns
   -- Set the auto-grow settings to grow based on megabytes instead of a percentage, so as to have the auto growth consistent

DBCC TRACE0FF (3226, -1)


-- Create two Databases 
-- Create database auto
-- Create database auto2

-- DROP AutoDB => In case is already created to start new demo
Use master
Exec msdb.dbo.sp_delete_database_backuphistory @database_name = N'AutoDB'

Use master
Alter Database [AutoDB] Set Single_User With Rollback Immediate

Use master
Drop Database [AutoDB]


-- DROP AutoDB2 => In case is already created to start new demo
Use master
Exec msdb.dbo.sp_delete_database_backuphistory @database_name = N'AutoDB2'

Use master
Alter Database [AutoDB2] Set Single_User With Rollback Immediate

Use master
Drop Database [AutoDB2]

--Create database with default setting based on the Model database configuration
-- Create Database AutoDB and AutoDB2

Use master
GO

CREATE DATABASE [AutoDB]
 CONTAINMENT = NONE
 ON  PRIMARY 
( NAME = N'AutoDB', 
FILENAME = N'E:\MSSQL\AutoDB.mdf' , 
SIZE = 3072KB ,      --<< initial size of data file 3mb      
FILEGROWTH = 1024KB )  --<< growth by 1megabyte

 LOG ON 
( NAME = N'AutoDB_log', 
FILENAME = N'F:\MSSQL\AutoDB_log.ldf' , 
SIZE = 1024KB ,    --<< initial size of log file 1mb 
FILEGROWTH = 10%)  --<< growth by 10%
GO

Use AutoDB
DBCC LogInfo; -- To check how many VIRTUAL LOG FILES => 4


--Create database with set LOG FILE setting

Use master
CREATE DATABASE AutoDB2
 CONTAINMENT = NONE
 ON  PRIMARY 
( NAME = N'AutoDB2', 
FILENAME = N'E:\MSSQL\AutoDB2.mdf' , 
SIZE = 1024000KB ,       --<< initial size of data file 1000mb 
FILEGROWTH = 102400KB )  

 LOG ON 
( NAME = N'AutoDB2_log', 
FILENAME = N'F:\MSSQL\AutoDB2_log.ldf' , 
SIZE = 102400KB , 
FILEGROWTH = 102400KB ) --<< growth by 100mb (PRE SIZED SO THAT THE AUTO GROWTH DOES NOT ACTIVATE)
GO

Use AutoDB2
DBCC LogInfo -- To check how many VIRTUAL LOG FILES

-- examine the database files


sp_helpdb AutoDB
-- name         field    filename                   filegroup   size       maxsize          growth     usage
-- AutoDB	    1	     E:\MSSQL\AutoDB.mdf 	    PRIMARY	    8192 KB	   Unlimited	    1024 KB	   data only
-- AutoDB_log	2	     F:\MSSQL\AutoDB_log.ldf	NULL	    1024 KB	   2147483648 KB	10%	log    only


sp_helpdb AutoDB2
-- name         field    filename                   filegroup   size           maxsize          growth       usage
-- AutoDB2	    1	     E:\MSSQL\AutoDB2.mdf 	    PRIMARY	    1024000 KB	   Unlimited	    102400 KB	 data only
-- AutoDB2_log	2	     F:\MSSQL\AutoDB2_log.ldf	NULL	    102400 KB	   2147483648 KB	102400 KB    only

dbcc sqlperf (logspace) -- Initial log space use and allocated
-- Database Name      Log Size(mb)    Log Space Used(%)     Status
-- AutoDB	          0.9921875	      41.14173	            0
-- AutoDB2	          99.99219	      0.5068755	            0

-- Backup Database AutoDB
Use AutoDB
BACKUP DATABASE AutoDB
TO DISK = 'E:\MSSQL\AutoDB_Full.bak';

--move data from adventureworks2016 to AutoDB and AutoDB2 database via import/export wizard GUI method  OR 
Use AutoDB
Select * Into LogGrowthTable from adventureworks2016.sales.SalesOrderDetail

sp_helpdb AutoDB
-- name         field    filename                   filegroup   size           maxsize          growth     usage
-- AutoDB	    1	     E:\MSSQL\AutoDB.mdf 	    PRIMARY	    19456 KB	   Unlimited	    1024 KB	   data only
-- AutoDB_log	2	     F:\MSSQL\AutoDB_log.ldf	NULL	    13632 KB	   2147483648 KB	10%	log    log only



dbcc sqlperf (logspace) -- Initial log space use and allocated
-- Database Name      Log Size(mb)    Log Space Used(%)     Status
-- AutoDB	          13.30469	      97.16676	            0
-- AutoDB2	          99.99219	      0.5068755	            0


-- Backup Database AutoDB
Use AutoDB
BACKUP DATABASE AutoDB
TO DISK = 'E:\MSSQL\AutoDB_Full.bak';


BACKUP log  AutoDB
TO DISK = 'F:\MSSQL\AutoDB_log.trn'

sp_helpdb AutoDB
-- name         field    filename                   filegroup   size           maxsize          growth     usage
-- AutoDB	    1	     E:\MSSQL\AutoDB.mdf 	    PRIMARY	    19456 KB	   Unlimited	    1024 KB	   data only
-- AutoDB_log	2	     F:\MSSQL\AutoDB_log.ldf	NULL	    13632 KB	   2147483648 KB	10%	log    log only

-------------------------------------------------------------------------

-- Backup Database AutoDB2
Use AutoDB2
BACKUP DATABASE AutoDB2
TO DISK = 'E:\MSSQL\AutoDB2_Full.bak';

--move data from adventureworks2016 to AutoDB and AutoDB2 database via import/export wizard GUI method  OR 
Use AutoDB2
Select * Into LogGrowthTable from adventureworks2016.sales.SalesOrderDetail

sp_helpdb AutoDB2
-- name         field    filename                   filegroup   size           maxsize          growth       usage
-- AutoDB2	    1	     E:\MSSQL\AutoDB2.mdf 	    PRIMARY	    1024000 KB	   Unlimited	    102400 KB	 data only
-- AutoDB2_log	2	     F:\MSSQL\AutoDB2_log.ldf	NULL	    102400 KB	   2147483648 KB	102400 KB    only

dbcc sqlperf (logspace) -- Initial log space use and allocated
-- Database Name      Log Size(mb)    Log Space Used(%)     Status
-- AutoDB	          13.30469	      97.16676	            0
-- AutoDB2	          99.99219	      12.64112	            0


-- Backup Database AutoDB2
Use AutoDB2
BACKUP DATABASE AutoDB2
TO DISK = 'E:\MSSQL\AutoDB2_Full.bak';

BACKUP log  AutoDB2
TO DISK = 'F:\MSSQL\AutoDB2_log.trn'


sp_helpdb AutoDB2
-- name         field    filename                   filegroup   size           maxsize          growth       usage
-- AutoDB2	    1	     E:\MSSQL\AutoDB2.mdf 	    PRIMARY	    1024000 KB	   Unlimited	    102400 KB	 data only
-- AutoDB2_log	2	     F:\MSSQL\AutoDB2_log.ldf	NULL	    102400 KB	   2147483648 KB	102400 KB    only


------------------------------------------------------------------------
--RESULTS:

-- As the insert is being recored in the transaction log that was 1mb in size, the initial size (1mb) of the tlog recording can't keep up with the
-- activity, and as such, needs to expand by 10% each time there is modifications to record.

-- Below is query to find auto growth setting for all or specified database (or use the SQL reports)
-- GUI TO SHOW DISK USAGE => Right click on the DATABASENAME (AUTODB) - REPORTS - STANDARD REPORT - DISK USAGE - EXPAND + (Data/Log Files Autogrow/Autoshrink Events -- There you will see all the details just like the output from the query below


USE [master]
GO
 
BEGIN TRY
    IF (SELECT CONVERT(INT,value_in_use) FROM sys.configurations WHERE NAME = 'default trace enabled') = 1
    BEGIN
        DECLARE @curr_tracefilename VARCHAR(500);
        DECLARE @base_tracefilename VARCHAR(500);
        DECLARE @indx INT;
 
        SELECT @curr_tracefilename = path FROM sys.traces WHERE is_default = 1;
        SET @curr_tracefilename = REVERSE(@curr_tracefilename);
        SELECT @indx  = PATINDEX('%\%', @curr_tracefilename) ;
        SET @curr_tracefilename = REVERSE(@curr_tracefilename) ;
        SET @base_tracefilename = LEFT( @curr_tracefilename,LEN(@curr_tracefilename) - @indx) + '\log.trc'; 
        SELECT
            --(DENSE_RANK() OVER (ORDER BY StartTime DESC))%2 AS l1,
            ServerName AS [SQL_Instance],
            --CONVERT(INT, EventClass) AS EventClass,
            DatabaseName AS [Database_Name],
            Filename AS [Logical_File_Name],
            (Duration/1000) AS [Duration_MS],
            CONVERT(VARCHAR(50),StartTime, 100) AS [Start_Time],
            --EndTime,
            CAST((IntegerData*8.0/1024) AS DECIMAL(19,2)) AS [Change_In_Size_MB]
        FROM ::fn_trace_gettable(@base_tracefilename, default)
        WHERE
            EventClass >=  92
            AND EventClass <=  95
            --AND ServerName = @@SERVERNAME
            --AND DatabaseName = 'myDBName'  
			AND DatabaseName IN ('AutoDB','AutoDB2') -- Provide the database name
        ORDER BY DatabaseName, StartTime DESC;  
    END    
    ELSE   
        SELECT -1 AS l1,
        0 AS EventClass,
        0 DatabaseName,
        0 AS Filename,
        0 AS Duration,
        0 AS StartTime,
        0 AS EndTime,
        0 AS ChangeInSize 
END TRY 
BEGIN CATCH 
    SELECT -100 AS l1,
    ERROR_NUMBER() AS EventClass,
    ERROR_SEVERITY() DatabaseName,
    ERROR_STATE() AS Filename,
    ERROR_MESSAGE() AS Duration,
    1 AS StartTime, 
    1 AS EndTime,
    1 AS ChangeInSize 
END CATCH

-- The OUTPUT RESULT -- DATA/LOG FILES AUTOGROW/AUTOSHRINK EVENTS shows that AUTODB mdf is growing by 1mb with the default setting as you can see below and this is what will want to avoid as it cause FRAGMENTATION and the log file is growing by 0.38, 0.31,0.25.... And is causing FRAGMENTATION
-- The OUTPUT for AUTODB2 -- has no DATA/LOG FILES AUTOGROW/AUTOSHRINK EVENTS you notice that because will PRE-SIZED we dont have auto growth increments and that saves a tremendous amount of performance issues


-- DROP AutoDB 
Use master
Exec msdb.dbo.sp_delete_database_backuphistory @database_name = N'AutoDB'

Use master
Alter Database [AutoDB] Set Single_User With Rollback Immediate

Use master
Drop Database [AutoDB]


-- DROP AutoDB2 
Use master
Exec msdb.dbo.sp_delete_database_backuphistory @database_name = N'AutoDB2'

Use master
Alter Database [AutoDB2] Set Single_User With Rollback Immediate

Use master
Drop Database [AutoDB2]

-- SCRIPT TO CLEAR SQL SERVER TRANSACTION  LOG TO REGAIN SPACE
Use AutoDB2
Alter Database AutoDB2 Set Recovery Simple
GO

DBCC ShrinkFile (AutoDB2_log, 5)
GO

Alter Database AutoDB2 Set recovery Full
GO


-- DBCC LogInfo;


-- When you not pre-sized the auto growth settings and there is transaction that needs extra space the transaction log will constantly expand by the determined model size and that causes fragmentation, internal fragmentation. And we want to avoid that at all cost


-- 21. TYPES OF RECOVERY MODELS
------------------------------------------------------------------------------------------------------------------------------------------------

-- What are the recovery models and their roles?
   
   -- The recovery models in SQL Server, Simple, Full, Bulk-Logged, determine whether a transaction log backup is available or not
      
	  -- 1. With Simple recovery model
            -- Transaction log backups are not supported
            -- Truncation of the transaction log is done automatically, thereby releasing space to the system
            -- You can lose data, as there are no transaction log backups
            -- When in simple recovery mode, data in the T-Log will not grow
			-- NOTE! We do not have SIMPLE RECOVERY MODE in PRODUCTION database, while its easier to manage, we do not recommend having a production databases in SIMPLE mode.

      -- 2. With Bulk-logged recovery model
            -- Supports transaction log backups 
            -- As in Full mode, there is no automated process of transaction log truncation your space will full if you dont take T.LOG backups, T.LOG will continue to grow until it consume the entire space.
            -- Used primarily for bulk operations, such as bulk insert, thereby minimal recording it.

      -- 3. Full recovery model (Most prominent model that normally use in production base database environment)
            -- Supports transaction log backups(meaning if you got your database in full recovery model then you can take T.LOG backups of your transaction files and when you doing that it will clear the inactive portion of your transaction file, thereby controlling it 
			   -- When you take full database backup it records everything up to that point. 
			   -- When you take Transaction backups it takes snapshots of everything that is changed since the last full backup
            -- Little chance of data loss under the normal circumstances
            -- No automated process to truncate the log thus must have T-log backups it has to be done manually
            -- The transaction log backups must be made regularly to mark unused space available for overwriting means Hey am going to take a backup of this transaction log file and everything that is inactivepush it out and bring it to me
            -- When using Full recovery mode, the T-log can get large in size 
            -- During most backup processes, the changes contained in the logfile are sent to the backup file


-- RECOVERY MODELS
   -- There are three recovery models that can be set on each user database which determines the types of backups you’ll use. 
   -- You set the recovery model via the GUI or use the ALTER DATABASE command:
         
		 SELECT name, recovery_model_desc FROM sys.databases --ß find the recovery model
         
		 ALTER DATABASE SALES SET RECOVERY FULL
         
		 ALTER DATABASE SALES SET RECOVERY SIMPLE
         
		 ALTER DATABASE SALES SET RECOVERY BULK_LOGGED

-- FULL
   -- In full recovery you must run a log backup on a regular basis in order to reclaim log space. Recovery to a point in time is fully supported. For any production system that has data that is vital to the business, full recovery should be used.

ALTER DATABASE SALES SET RECOVERY FULL


-- BULK-LOGGED
  -- This recovery model reduces the size of the transaction log by minimally logging some operations such as bulk inserts. The problem is, recovery to a point in time is not possible with this recovery model because any minimally logged operations cannot be restored. This means that bulk-logged has all the overhead of Full Recovery, but effectively works like Simple Recovery. Unless you have specific needs or are willing to perform lots of extra backup operations, it’s not recommended to use bulk-logged recovery.

ALTER DATABASE SALES SET RECOVERY BULK_LOGGED


-- SIMPLE
   -- When in this mode, the transaction are removed automatically at each checkpoint within the database and no log backups are possible. Recovery to a point in time is not possible and you could lose substantial amounts of data under simple recovery.  Not advised for production databases that are critical.
ALTER DATABASE SALES SET RECOVERY SIMPLE



-- FIND THE RECOVERY MODE
Select name, recovery_model_desc
From sys.databases
Order by name

-- OUTPUT
--  name                -- recovery_model_desc
	AdventureWorks2019	   SIMPLE
	master	               SIMPLE
	model	               FULL
	msdb	               SIMPLE
	Sales	               FULL
	tempdb	               SIMPLE

-- LOOK AT THE BACKUP HISTORY WITH THIS SCRIPT
Use msdb
Go
Select Logical_Name, Physical_Name, File_Number, Backup_Size, File_Type, * 
From dbo.BackupFile
Order By 1 -- Contains one row for each data or log file that is backed up





-- Create Database Admin
Use Master
Create database Admin

-- Scripts To See All DBs Recovery Models for each database
select [name], DATABASEPROPERTYEX([name],'recovery') As RecoveryMode
from sysdatabases
where name not in ('master','model','tempdb','msdb') --< Excluding system databases

-- CHANGE THE RECOVERY MODE FOR DATABASE
   
   -- SET RECOVERY MODEL TO SIMPLE
Use master
Alter Database Admin
Set Recovery Simple  -- Now is SIMPLE

select [name], DATABASEPROPERTYEX([name],'recovery') As RecoveryMode
from sysdatabases
where name not in ('master','model','tempdb','msdb') --< Excluding system databases

   -- SET RECOVERY MODEL TO FULL
Use master
Alter Database Admin
Set Recovery Full -- Set it back to FULL

-- Now is in FULL 
select [name], DATABASEPROPERTYEX([name],'recovery') As RecoveryMode
from sysdatabases
where name not in ('master','model','tempdb','msdb') --< Excluding system databases


-- There is a function called (fn_dblog) which requires a beginning LSN and an ending LSN (Log Sequence Number) for a transaction, but for our purpose we will use NULL as starting and ending values. The following example will show how the transaction log records multiple rows of data for any activity against the database and how a transaction log backup truncates the transaction file, once it’s written to the backup log.  (fn_dblog)  A ways to view the contents of the log.  
-- DO NOT RUN THIS COMMAND IN PRODUCTION AS IT IS UNDOCUMENTED.


-- The following example will show how TRANSACTION LOG records multiple  rows of data for any activity against the database and how a transaction log backup truncates the transaction log file, once its written to the backup log file

-- LETS DROP THESE DATABASE FIRST AND START FRESH
Use master
Exec msdb.dbo.sp_delete_database_backuphistory @database_name = N'ViewLog'

Use master
Alter Database [ViewLog] Set Single_User With Rollback Immediate

Use master
Drop Database [ViewLog]


-- Now lets create new database [ViewLog]

Use master
Create Database ViewLog

-- Create Table => Cities
Use [ViewLog]
Create Table Cities
(City varchar (20));


-- View the inside of the transaction logs and view its activity for creating a database and a table


Select count(*) from fn_dblog (null, null) -- The two null values represent the starting and ending points of the log sequence number or LSN number -- 95 records of modifications

Use ViewLog
Select count(*) from fn_dblog (null, null) -- Results => 134 entries

-- Taking Full Backup of the database
Use ViewLog
Backup database ViewLog 
To Disk = N'E:\MSSQL\ViewLog.bak'


-- Insert Records into the table CITIES we created before
Insert into Cities 
Values ('New York')
Go 1000

-- Count how many modifications recorded in the transaction log file
Use ViewLog
Select count(*) from fn_dblog (null, null) -- 3059 entries

-- Backuping up FULL BACKUP of a database DOES NOT truncate the transaction log (Still many record in the fn_dblog) 
Use ViewLog
Backup database ViewLog 
To Disk = N'E:\MSSQL\ViewLog.bak'

-- Count how many modifications recorded in the transaction log file
Use ViewLog
Select count(*) from fn_dblog (null, null) -- 3115 entries now it increase bcos we backup the table


-- Backing up the TRANSACTION LOG file, truncate the transaction log file by moving the commited transaction to the backup file!
Use ViewLog
Backup Log ViewLog 
To Disk = N'F:\MSSQL\ViewLog.trn'


-- Count how many modifications recorded in the transaction log file
Use ViewLog
Select count(*) from fn_dblog (null, null) -- 53 entries now it increase bcos we backup the table

Use ViewLog
Select * from fn_dblog(null, null) -- Show begin and end operation


-- 23. WHAT IS VIRTUAL LOG FILE(VLF)
------------------------------------------------------------------

-- Virtual Log File and the transaction log file
-- Virtual Log Files (VLF)
-- Anatomy of a transaction log file --VLF blocks

-- The size and number of VLF added at the time of expanding the transaction log is based on this following criteria:

-- Transaction log size less than 64MB and up to 64MB = 4 VLFs
-- Transaction log size greater(larger) than 64MB and up to 1GB = 8 VLFs
-- Transaction size log greater(larger) than 1GB = 16 VLFs

-- 1. CREATE A DATABASE WITH LOG FILE LESS THAN 64 MB THAT WILL CREATE 4 VLFS

/*
The following will show that improper sizing of the transaction log file and setting and relying on the default auto growth contributes to internal log fragmentation, and causes the VLFS to increase.
*/

-- Note that the transaction log is 1 megabyte in size, and the autogrowth is set to grow in increments of 10% (bad practice)

Use master
CREATE DATABASE [Log Growth]
ON PRIMARY
( NAME = N'LogGrowth', FILENAME = N'E:\MSSQL\LogGrowth.mdf' ,
SIZE = 4096KB , -- 4 megabyte
FILEGROWTH = 1024KB )

LOG ON
( NAME = N'LogGrowth_log', FILENAME = N'F:\MSSQL\LogGrowth_log.ldf' ,
SIZE = 1024KB , -- 1 megabyte
FILEGROWTH = 10%)
GO

USE [Log Growth]
GO
-- Each row indicates a VLF

DBCC LOGINFO
GO

-- 4 VLFS
-- look at the size of the database and note transaction log is 1 MB and data file is 4MB

sp_helpdb [Log Growth] -- To see the mdf and ldf location and other properties

-- Insert data into table from another database and view the transaction log size, data file size, and the percentage of transaction log used

Use [Log Growth]
Select * Into LogGrowthTable from adventureworks2016.sales.SalesOrderDetail


select count(*) from LogGrowthTable -- Nr column name 121317

-- log space used 28.7 %

DBCC sqlPerf (LogSpace) -- Log Growth => Log Size(MB)13.30469  Log Space Used(%) 97.16676 Status 0

-- Look at the size of the database and note transaction log is 14 MB and data file is 15MB

sp_helpdb [Log Growth]

-- Each row indicates a VLF

DBCC LOGINFO
GO

-- 47 VLFs created as a result of improper pre sizing of the transaction log, and relying upon the auto growth property to accommodate the expansion of file

-- Drop the database

EXEC msdb.dbo.sp_delete_database_backuphistory @database_name = N'LogGrowth'
GO
USE [master]
GO
DROP DATABASE [Log Growth]
GO
-- ===============================================================================================================================
-- The following demonstration will illustrate the number of VLF created depending upon the sizing of the transaction log
-- Inserting the same amount of data into the table, 
-- but this time sizing the transaction log before inserting data by managing the autogrowth size so as to avoid VLFS from being created

-- transaction log size larger than 64MB and up to 1GB = 8 VLFs

CREATE DATABASE [Log Growth]
ON PRIMARY
( NAME = N'LogGrowth', FILENAME = N'E:\MSSQL\LogGrowth.mdf' ,
SIZE = 1000 MB ,
FILEGROWTH = 100 MB )

LOG ON
( NAME = N'LogGrowth_log', FILENAME = N'F:\MSSQL\LogGrowth_log.ldf' ,
SIZE = 500 MB , -- 500 MEGA BYTES - PRE SIZING THE LOG FILE SO AS TO AVOID AUTO GROWTH FROM KICKING IN. AUTOGROWTH SET TO 100 GROWTH RATE AS A FAILSAFE
FILEGROWTH = 100 MB) -- file auto growth NOT set to 10%, but allocated 100 in MEGA BYTES
GO

USE [Log Growth]
GO
-- Each row indicates a VLF

DBCC LOGINFO -- 8 VLFS
GO 

-- Look at the size of the database and note transaction log is 500 MB and data file is 1000

sp_helpdb [Log Growth]

-- Insert data into table from another database and view the transaction log size, data file size, and the percentage of transaction log used

Use [Log Growth]
go

Select * Into LogGrowthTable from adventureworks2016.sales.SalesOrderDetail

select count(*) from LogGrowthTable

-- 121317

-- Log space used 2.504434 %

DBCC sqlPerf (LogSpace)  -- OLD -- Log Growth => Log Size(MB)13.30469  Log Space Used(%) 97.16676 Status 0
                         -- NEW -- Log Growth => Log Size(MB)499.9922  Log Space Used(%) 2.524844 Status 0

-- Each row indicates a VLF

DBCC LOGINFO
GO
-- VLFs have not increased in number as a result of pre sizing the transaction log to 500 MB and therefore having the need to rely on auto growth from kicking in
-- Drop the database

EXEC msdb.dbo.sp_delete_database_backuphistory @database_name = N'Log Growth'
GO
USE [master]
GO
DROP DATABASE  [Log Growth]  
GO

-- 24. CREATE A SQL DATABASE
--------------------------------------------------------------------------

-- Create database either by GUI or Script it out

-- Create a database with proper configuration taking in the following consideration:

-- Pre size the data file and auto growth property of the data file
-- Pre size the log file and auto growth property of the log file
-- Have each data and transaction log file on its own physical drive
-- Sizing based on the activity that your environment uses
-- Base your size roughly on two or three years of growth patterns

-- NOTE: CHANGED THE DEFAULT KB TO MG!

USE master
GO

CREATE DATABASE [Production2]
 CONTAINMENT = NONE
 ON  PRIMARY 
( NAME = N'Production2', 
FILENAME = N'E:\MSSQL\Production2.mdf' , --<< this must be in its own physical drive
SIZE = 5MB , --<< pre size the data file 5000 mb
FILEGROWTH = 1MB )  --<< pre size the log file 100 mb

 LOG ON 
( NAME = N'Production2_log', 
FILENAME = N'F:\MSSQL\Production2_log.ldf' , --<< this must be in its own physical drive
SIZE = 1MB , --<< pre size size of transaction log file 100 mb
FILEGROWTH = 1MB ) --<< pre size size auto growth to increase in increments of 5 mb
GO


sp_helpdb [Production]

-- 1 minute and 10 seconds to create a database for database file size 5 gigs!

-- Size the physical DRIVE size of both the data and the transaction log file based on activity and workload
-- as both the data and log files are on seperate physical hard drives, if possible, allocated the data file and log file to the amount of physical drive!
-- We have set the auto growth to increase by 50 mg for the transaction log, but what if a transaction needs more than 50 mg of space, won't that cause
-- the auto growth to kick in?
-- and if another transaction needs more than 50mg of space, won't the transaction log get full and stop working?
-- YES
-- but you must take transaction log backups to truncate the transaction log file so that it does not consume the physical drive!!!



-- 25.THE IMPORTANCE OF TEMPDB
--------------------------------------------------------------------------------
-- What is the SQL Server TempDB
   -- SQL Server TempDB is a system database that is automatically created when you install SQL Servr or each time you start the SQL service
   
-- SQL TempDB Purpose
  -- Its used by SQL Server and user's, its a work horse
  -- Creation of Global or local temporary tables
  -- Table variables
  -- Use the tempdb for sorting index when using the SORT_IN_TEMPDB in your CREATE INDEX statement to off load the main database files, thus improving performance
  -- MARS - (Multiple Active Result Sets)
  -- Snapshot isolation and Read-Committed Snapshot Isolation
  -- Very busy database that needs attention



  -- DOS and Don't for the TEMPDB
     
  -- DON'T
	    -- You can not drop it
		-- Back it up
		-- Change its recovery model
		-- Can not create multiple file groups
		-- Detach it
		-- Backup and restore

   -- DOS
        -- Place both the data and the log file on a separate drive(disk)
		-- Place the files on the fastest IO subsystem possible
		-- Pre size auto gro, and pre size both the data and log files
		-- Create multiple data files for tempdb according to the 'formula' 1 data file per physical or virtual CPU core
		-- Keep the sizes of files equal

-- FINDING THE PATH AND MOVING THE TEMPDB TO ITS OWN DRIVE
---------------------------------------------------------------------------------------------------------------
   
   -- FIND THE PATH TO TEMPDB
---------------------------------------------------------------------

-- TO SEE THE PATH TO TEMPDB
---------------------------------------------------------------------------------------------------------------
   sp_helpdb tempdb -- To see the file location and its properties

-- name		    filename
-- tempdev		C:\Program Files\Microsoft SQL Server\MSSQL11.MSSQLSERVER\MSSQL\DATA\tempdb.mdf
-- templog		C:\Program Files\Microsoft SQL Server\MSSQL11.MSSQLSERVER\MSSQL\DATA\templog.ldf

-- TO CHANGE THE TEMPDB LOCATION USE THE FOLLOWING SCRIPT:
USE master;
GO

ALTER DATABASE tempdb
MODIFY FILE
(NAME = tempdev, FILENAME = 'G:\MSSQL\tempdb.mdf');
GO

ALTER DATABASE tempdb
MODIFY FILE
(NAME = templog, FILENAME = 'H:\MSSQL\templog.ldf');
GO

ALTER DATABASE tempdb
MODIFY FILE
(NAME = temp2, FILENAME = 'G:\MSSQL\tempdb_mssql_2.ndf');
GO

-- OUTPUT
-- The file "tempdev" has been modified in the system catalog. The new path will be used the next the database is started
-- The file "templog" has been modified in the system catalog. The new path will be used the next the database is started
-- The file "temp2" has been modified in the system catalog. The new path will be used the next the database is started

-- So you need to restart the INSTANCE - Right click on the server name - Restart - it will restart the MSSQLSERVICE on SQL01 and the AGENT
-- CHECK
sp_helpdb tempdb

-- AFTER 
-- tempdev	1	G:\MSSQL\tempdb.mdf	         PRIMARY	8192 KB 	Unlimited	65536 KB	data only
-- templog	2	H:\MSSQL\templog.ldf	     NULL	    8192 KB 	Unlimited	65536 KB	log only
-- temp2	3	G:\MSSQL\tempdb_mssql_2.ndf	 PRIMARY	8192 KB 	Unlimited	65536 KB	data only





-- YOU CAN NOT CHANGE THE RECOVERY MODEL FROM SIMPLE TO FULL (NOT POSSIBLE)

USE [master]
GO
ALTER DATABASE [tempdb] SET RECOVERY FULL 
GO

-- SIZING THE DATA AND LOG FILES AND AUTOGROWTH PROPERTY
-- SCRIPT TO RESIZE DATA AND LOG FILES AND AUTOGROWTH

USE [master]
GO
ALTER DATABASE [tempdb] 
MODIFY FILE ( NAME = N'tempdev', SIZE = 102400KB , FILEGROWTH = 20480KB )--<< pre sizing data, auto growth

GO
ALTER DATABASE [tempdb] 
MODIFY FILE ( NAME = N'templog', SIZE = 20480KB , FILEGROWTH = 20480KB )--<< pre sizing log, auto growth
GO

-- 26. ATTACH AND DETACH A DATABASE
---------------------------------------------------------------------------

-- What is detaching and attaching a database
   -- The process of moving the data and the log files to another drive or server for performance reasons 

-- How to use detach and attach a database
   -- At times there may be a need to move a database to another physical drive or another physical server; if so, you can use the sprocs detach and attach or use a GUI

-- Step by step moving a database
   
-- The following scripts will detach and then re-attach the sales database

-- Find the path of the database

sp_helpdb sales

-- Set the database to a single user mode

USE [master]
GO

ALTER DATABASE [Sales] 
SET SINGLE_USER WITH ROLLBACK IMMEDIATE
GO

-- Detach the database using the sprocs

USE [master]
GO

EXEC master.dbo.sp_detach_db @dbname = N'Sales', @skipchecks = 'false'
GO

-- Reattach the database using the FOR ATTACH command

USE [master]
GO
CREATE DATABASE [Sales] ON 
( FILENAME = N'C:\Program Files\Microsoft SQL Server\MSSQL11.MSSQLSERVER\MSSQL\DATA\Sales.mdf' ),
( FILENAME = N'C:\Program Files\Microsoft SQL Server\MSSQL11.MSSQLSERVER\MSSQL\DATA\Sales_log.ldf' )
 FOR ATTACH --<<use for attach to attach the sales database
GO

-- Note: When moving a database using detach and attach, you will lose the users in the original database after the completion of the move to a new server.  To resolve this, use the following sproc to link the user to the login in the new server. means if will MIGRATE database from one server to anither server or to a new location we will loose USERS on the database

-- USE THIS SPROC TO UPDATE THE USERS
EXEC sp_change_users_login 'Update_One', 'Bob', 'Bob'



-- 27. DATABASE BACKUP STRATEGIES
---------------------------------------------------------------------------

-- One of the most important responsibilities of a DBA is the planning, testing and deployment of the backup plan.  
-- The planning of the backup strategy, ironically, starts with the restore strategy that has been determined by the organization.   
   
   -- Two leading questions direct the restore strategy: 
      -- What is the maximum amount of data loss that can be tolerated?
	  
	  -- And what is the accepted time frame for the database to be restored in case of a disaster.
         
		 -- ANSWER => The first question involves taking multiple transaction logs backups and the second question involves the use of high availability solutions such as Database Miroring, LogShipping or High Availability Groups. To answer these question, you must ask the why, what, when, where and who for the strategy.


-- WHY BACK-UP?
   -- Backups are taken in order to be able to restore the database to its original state just before the disaster and protect against the following reasons:
      -- hardware failure
      -- malicious damage
      -- user-error
      -- database corruption

-- WHAT IS BACKUP?
   -- Simply put, it's an image of the database at the time of the full backup.

-- THERE ARE SEVERAL DIFFERENT KINDS OF BACKUPS:
   -- 1. Full backup - a complete, page-by-page copy of an entire database including all the data, all the structures and all other objects stored within the database.
   -- 2. Differential backups -  a backup of a database that have been modified in any way since the last full backup.
   -- 3. Transaction Log - a backup of all the committed transactions currently stored in the transaction log
   -- 4. Copy-only backup -  a special-use backup that is independent of the regular sequence of SQL Server backups.
   -- 5. File backup - a backup of one or more database files or filegroups
   -- 6. Partial backup - contains data from only some of the filegroups in a database


-- WHEN TO DO BACKUPS?
   -- Again, this is a company decision that must be determined by asking the following questions: 
   
-- What is the maximum amount of data loss that can be tolerated, and what is the accepted time frame for the database to be restored in case of a disaster?    -- The first question will address the use of transaction logs and their frequency, and the second question will involve some type of high availability solution.

-- WHERE SHOULD BACKUPS BE PLACED?
   -- Ideally, the backups should be placed on their own drives onto separate servers from the production server, and then using a third party application, copies made to a remote server on a separate site. What should not be tolerated is having backups on the same drive and server as the data and log files.  If the server crashes, we lose all hopes or restoring critical data!

-- WHAT NEEDS BACKING UP?
   -- All user defined databases and system database should be backed up.


-- WHO SHOULD PERFORM BACKUPS?
   -- The person in charge of the backup planning and executing will most likely be the Database Administrator (DBA).  He will coordinate with the upper management, direct and provide valuable advice on the best practices for restoring the database; however, note that on a production server, most of the backups will be automated by using the SQL Agent and jobs.


-- BACKUP RETENTION PERIOD
   -- The amount of backups retained is a question determined by the business needs. Most likely, it may involve a month of backups onsite and three months of backups offsite.  But again, this depends upon the organization needs.


-- PERFORMING BACKUPS
  -- The following method illustrates the use of T-SQL to backup database because it provides a greater and granular control of the process. 
  -- However, you can use the SSMS console.




-- FULL DATABASE BACKUPS
------------------------------------------------------------------------------

-- FULL DATABASE BACKUP
   -- Backup the whole database, which includes parts of transaction log which is needed to recover the database using full backup.

-- Always take FULL BACKUP during the off hour that is off the production hour something like 12 A.M or 1 A.M


-- BACKUP HISTORY
   -- The following commands provides information as to the history of the backups in the MSDB database.

Use msdb
go
SELECT * FROM dbo.backupfile  -- Contains one row for each data or log file that is backed up
SELECT * FROM dbo.backupmediafamily  -- Contains one row for each media family
SELECT * FROM dbo.backupmediaset -- Contains one row for each backup media set
SELECT * FROM dbo.backupset  -- Contains a row for each backup set
SELECT * FROM dbo.backupfilegroup -- Contains one row for each filegroup in a database at the time of backup


--BACKUP FILE INFORMATION
  -- It is possible to look at a backup file itself to get information about the backup. The header of the backup stores data like when the backup was created, what type of backup it is, the user who took the backup and all other sorts of information. The basic commands available are:

RESTORE LABELONLY FROM DISK = N'I:\MSSQL\sales.bak'
RESTORE HEADERONLY FROM DISK = N'I:\MSSQL\sales.bak'
RESTORE FILELISTONLY FROM DISK = N'I:\MSSQL\sales.bak'

-- BASIC BACKUP PROCESS
  -- The following  backup schedule is one that I have used on a production server for critical data bases.
	-- Sunday Night: Full Backup with database consistency check (DBCC CHECKDB)
	-- Monday-Saturday: Differential Backup each day
	-- Midnight-11:45PM: Log Backup every 30 minutes

-- What is important to understand is that you frequently test your backups with restores, at least once a month, to ensure that your backups are valid and restorable.  Having backups is useless unless they have been tested well.  When backing up database, starting with SQL version 2008 and up, you have the ability to compress the databases backup, which saves time to backup and restore.  Note that the CPU resource will consume about 20% additional resource, thus, the backup must be done in the off hours.
 
 
-- EXAMPLE OF BACKUP PLAN USING DIFFERENT TYPES OF BACKUPS
	-- 12:00 am - Create an empty database (20 gigs)
	-- 12:15 am - Full backup
	-- 12:30 am - Insert 100,000 rows (1 gig)
	-- 12:45 am - Differential backup (size1 gig after diff)
	-- 1:00  am - Insert 100,000 rows (1 gig)
	-- 1:15  am - Differential backup (size 2 gigs after diff)
	-- 1:30  am - Insert 100,000 rows (1 gig)
	-- 1:15  am - Differential backup (size 3 gigs after diff)
	-- 1:30  am - Insert 100,000 rows (1 gig)
	-- 1:45  am - Transactional log   (size 4 gigs after t-log)
	-- 2:00  am - Insert 100,000 rows (1 gig)
	-- 2:15  am - Transactional log   (size 5 gigs after t-log)
 
-- RESTORE THE DATABASE PROCESS
	-- 12:15 am - Full backup
	-- 1:15  am - Differential backup (size 3 gigs after diff)
	-- 1:45  am - Transactional log   (size 4 gigs after t-log)
	-- 2:15  am - Transactional log   (size 5 gigs after t-log)



-- GIU METHOD - Right click on the DATABASE_Name - Tasks - Backup - New Window pop up - Change the DESTINATION to the BACKUP folder - Remove - Add - Name the                BACKUP DATABASE eg I:\MSSQL\AdventureWorks2019_Full.bak - Media Option - Select - Append to the existing Backup set - Reliability - Select -                 Verify Backup when is finished - Ok

-- CREATE A DATABASE Sales

Use master
go
 
 
Create database Sales
go
 
use sales
go
 
-- CREATE TABLE CALLED PRODUCTS IN DATABASE SALES
Create table Products
(ProductID int IDENTITY (1,1) Primary Key,
ProductName varchar (100),
Brand varchar (100))
go

-- INSERT DATA INTO PRODUCTS TABLE
insert into Products values ('Bike','Genesis')
insert into Products values ('Hat','Nike')
insert into Products values ('Shoe','Payless')
insert into Products values ('Phone','Apple')
insert into Products values ('Book','Green')
insert into Products values ('Cup','Large')

-- VIEW THE TABLE PRODUCTS
select * from Products

-- TAKE FULL DATABASE BACKUP OF THE SIX ROWS
BACKUP DATABASE [sales]
TO  DISK = N'I:\MSSQL\sales.bak'
WITH NOINIT, -- NOINIT means it would not over written
NAME = N'sales-Full Database Backup', -- Name it how you want
COMPRESSION,
STATS = 5
GO -- This BACKUP ONLY HAS FIRST SIX ROW RECORDS

-- LETS ADD ANOTHER ROW 
-- INSERT DATA INTO PRODUCTS TABLE
insert into Products values ('Motor','Audi')
insert into Products values ('Boot','Adidas')

-- VIEW THE TABLE PRODUCTS
select * from Products -- 8 Records

-- TAKE FULL DATABASE BACKUP OF THE SIX ROWS
   -- BACKUP DATABASE [sales]
   -- TO  DISK = N'I:\MSSQL\sales.bak'
   -- WITH NOINIT, -- NOINIT means it would not over written
   -- NAME = N'sales-Full Database Backup',
   -- COMPRESSION,
   -- STATS = 5


-- VERIFY THAT THE DATABASE BACKUP IS VALID (not that the data within is valid)
declare @backupSetId as int
select @backupSetId = position
from msdb..backupset
where database_name=N'sales'
and backup_set_id=(select max(backup_set_id)
from msdb..backupset where database_name=N'sales' )
if @backupSetId is null
begin
raiserror(N'Verify failed. Backup information for database ''sales'' not found.', 16, 1)
end
RESTORE VERIFYONLY
FROM
DISK = N'I:\MSSQL\sales.bak'
WITH  FILE = @backupSetId
GO

-- YOU CAN LOOK AT THE BACKUP HISTORY WITH THIS SCRIPT
Use msdb
Go
Select Logical_Name, Physical_Name, File_Number, Backup_Size, File_Type, * 
From dbo.BackupFile
Order By 1 -- Contains one row for each data or log file that is backed up

-- LOOK INSIDE THE .bak FILE
Use Sales
Restore Headeronly From Disk = N'I:\MSSQL\Sales.bak'

-- VIEW THE TABLE PRODUCTS
select * from Products -- 8 Records

-- INSERT DATA INTO TABLE AFTER A FULL DATABASE BACKUP, BEFORE A TRANSACTION LOG BACKUP 
Insert Into Products Values ('Doll', 'Toy_R_us')

-- VIEW THE TABLE PRODUCTS
select * from Products -- 9 Records 
-- NOTE*** If will restore now it will only restore 8 records NOT 9 bcos we have not taking the any backup after the insertion of the 9th records





-- 29 TRANSACTIONAL LOG BACKUPS
--------------------------------------------------------------------------------------------------------
-- TRANSACTION LOG BACKUP
   -- Taking a TRANSACTION LOG backup (this can only be taken if the database recovery model is FULL mode, and a FULL database backup had been executed)
   -- You must backup the transaction log, if SQL Server database uses either FULL or BULK-LOGGED recovery model otherwise transaction log is going to full. Backing up the transaction log truncates the log and user should be able to restore the database to a specific point in time.

-- LETS TAKE ANOTHER TRANSACTION LOG BACKUP
Use Sales
BACKUP LOG [sales]
TO  DISK = N'I:\MSSQL\sales.bak'
WITH
NAME = N'sales-Transaction Log Backup',
COMPRESSION,
STATS = 10
GO

-- Look inside the .bak file
Use Sales
Restore Headeronly From Disk = N'I:\MSSQL\Sales.bak'



-- BACKUPTYPE
  -- 1 = Full 
  -- 2 = T.Log
  -- 3 = Differential

-- LETS VALID THAT => VERIFY THAT THE DATABASE BACKUP IS VALID (not that the data within is valid)
declare @backupSetId as int
select @backupSetId = position
from msdb..backupset
where database_name=N'sales'
and backup_set_id=(select max(backup_set_id)
from msdb..backupset where database_name=N'sales' )
if @backupSetId is null
begin
raiserror(N'Verify failed. Backup information for database ''sales'' not found.', 16, 1)
end
RESTORE VERIFYONLY
FROM  DISK = N'I:\MSSQL\sales.bak'
WITH  FILE = @backupSetId
GO

-- VIEW THE TABLE PRODUCTS
select * from Products -- 9 Records 
-- If will restore from FULL BACKUP and T.Log Backup we will only get 9 records bcos w never take another backup after insert new records

-- INSERT ANOTHER RECORDS 
Insert Into Products Values ('House', 'Remax')

-- VIEW THE TABLE PRODUCTS
select * from Products -- 10 Records 
-- If will restore from FULL BACKUP and T.Log Backup we will only get 9 records bcos w never take another backup after insert new records


-- LETS TAKE ANOTHER TRANSACTION LOG BACKUP

Use Sales
BACKUP LOG [sales]
TO  DISK = N'I:\MSSQL\sales.bak'
WITH
NAME = N'sales-Transaction Log Backup',
COMPRESSION,
STATS = 10
GO

-- LOOK INSIDE THE .bak file
Use Sales
Restore Headeronly From Disk = N'I:\MSSQL\Sales.bak' -- Now we have 1 time FULL BACKUP and 2 times TRANSACTION LOG BACKUP



-- LETS VALID THAT => VERIFY THAT THE DATABASE BACKUP IS VALID (not that the data within is valid)
declare @backupSetId as int
select @backupSetId = position
from msdb..backupset
where database_name=N'sales'
and backup_set_id=(select max(backup_set_id)
from msdb..backupset where database_name=N'sales' )
if @backupSetId is null
begin
raiserror(N'Verify failed. Backup information for database ''sales'' not found.', 16, 1)
end
RESTORE VERIFYONLY
FROM  DISK = N'I:\MSSQL\sales.bak'
WITH  FILE = @backupSetId
GO  -- Output Message => The backup set on file 3 is valid. Completion time: 2023-07-18T10:42:52.7499115+02:00




-- 30 DIFFERENTIAL BACKUP -- only records the changes since the last database full backup and is primarily best use in large database not really good use in small database, so in T.LOG is sufficient for small database
-------------------------------------------------------------------------------------------------------------------------------------------
  
   -- The database must have a full back up in order to take a differential backup; it only backups the changes since last full backup.
  
   -- LETS START FRESH WITH DIFFERENTIAL BACKUP means We are doing FULL BACKUP, TRANSACTION LOG BACKUP AND THEN DIFFERENTIAL BACKUP

-- DROP DATABASE SALES
	--Use master
	--Drop database Sales

Use master
Exec msdb.dbo.sp_delete_database_backuphistory @database_name = N'Sales'

Use master
Alter Database Sales Set Single_User With Rollback Immediate

Use master
Drop Database Sales

-- CREATE DATABASE GOODS
Use Master
Create Database Sales
Go


Use Sales
Go

-- CREATE TABLE
Create Table Products
(ProductID int Identity(1,1) Primary Key,
ProductName varchar(100),
Brand varchar(100))

-- INSERT DATA INTO PRODUCTS TABLE
insert into Products values ('Bike','Genesis')
insert into Products values ('Hat','Nike')
insert into Products values ('Shoe','Payless')
insert into Products values ('Phone','Apple')
insert into Products values ('Book','Green')
insert into Products values ('Cup','Large')

-- VIEW DATA
Select * From Products -- 6 records present

-- TAKE FULL DATABASE BACKUP OF THE SIX ROWS DATA INSERTED
BACKUP DATABASE Sales
TO  DISK = N'I:\MSSQL\sales.bak'
WITH  NOINIT, -- No Override
NAME = N'sales-Full Database Backup',
STATS = 10
GO

-- LOOK INSIDE THE .bak file
Restore HeaderOnly From Disk = N'I:\MSSQL\Sales.bak' -- Six records present

-- INSERT ONE MORE ROW DATA INTO THE TABLLE AFTER A FULL DATABASE BACKUP, BUT BEFORE A TRANSACTION LOG BACKUP
insert into Products values ('Doll','Toy_R_us')

-- VIEW DATA
Select * From Products  -- 7 records present

-- TAKING A TRANSACTION LOG BACKUP (this can only be taken if the database recovery model is in FULL MODE, and a FULL database backup had been executed)
BACKUP LOG [sales]
TO  DISK = N'I:\MSSQL\sales.bak'
WITH
NAME = N'sales-Transaction Log  Backup',
STATS = 10
GO

-- LOOK INSIDE THE .bak file
Restore HeaderOnly From Disk = N'I:\MSSQL\Sales.bak' -- 1 Time FULL and 1 Time T.LOG (2 RECORDS)

-- INSERT ANOTHER DATA
Insert into Products values ('House', 'Remax')

-- VIEW DATA
Select * From Products -- 8 records

-- TAKING ANOTHER TRANSACTION LOG BACKUP
BACKUP LOG [sales]
TO  DISK = N'I:\MSSQL\sales.bak'
WITH
NAME = N'sales-Transaction Log  Backup',
STATS = 10
GO

-- LOOK INSIDE THE .bak file
Restore HeaderOnly From Disk = N'I:\MSSQL\Sales.bak' -- 1 Time FULL and 2 Time T.LOG (3 RECORDS)


-- INSERT ANOTHER DATA
Insert into Products values ('Car', 'Porche')


-- VIEW DATA
Select * From Products -- 9 records

-- This is where DIFFERENTIAL BACKUP comes in place bcos if you keep doing T.LOG backup of a database you will keep doing T.LOG backup every 15min and you w have more than 99 T.LOG within a day which is bad so taking DIFFERENTIAL backup help to backup database since the last FULL BACKUP. To reduce several T.LOG and to reduce use of storage space.

-- NOW LETS TAKE DIFFERENTIAL BACKUPS 
------------------------------------------------------
BACKUP Database [sales]
TO  DISK = N'I:\MSSQL\sales.bak'
WITH DIFFERENTIAL, 
NOINIT, -- No override
NAME = N'sales-Differential Database  Backup',
STATS = 10
GO


-- INSERT ANOTHER RECORDS 
Insert Into Products Values ('Chair', 'Walmart') -- 10 records


-- TAKING ANOTHER T.LOG BACKUP
Backup Log Sales
To Disk = N'I:\MSSQL\Sales.bak'
With
Name = N'Sales-Transaction Log Backup',
STATS = 10
Go



-- LOOK INSIDE THE .bak file
Restore HeaderOnly From Disk = N'I:\MSSQL\Sales.bak'


-- INSERT ANOTHER DATA
Insert Into Products Values ('Mouse', 'Apple')


-- TAKING ANOTHER T.LOG
Backup Log Sales
To Disk = N'I:\MSSQL\Sales.bak'
With 
Name = N'Sales-Transaction Log Backup',
STATS = 10
Go

-- INSERT ANOTHER DATA
Insert Into Products Values ('TV', 'Sony')

-- VIEW DATA
Select * From Products  -- 12 Records


-- TAKING BACKUP DIFFERENTIAL 2ND TIME -- remember differential only records the changes since the last full backup
Backup Database Sales
To Disk = N'I:\MSSQL\Sales.bak'
With Differential,
Name = N'Sales-Differential Database backup', 
STATS = 10
Go

-- LOOK INSIDE THE .bak file
Restore HeaderOnly From Disk = N'I:\MSSQL\Sales.bak'

--OUTPUT BELOW
    --BackupName                                -- BackupType
--sales-Full Database Backup	           NULL  	1
--sales-Transaction Log  Backup	           NULL	    2
--sales-Transaction Log  Backup	           NULL	    2
--sales-Differential Database  Backup	   NULL  	5
--Sales-Transaction Log Backup	           NULL  	2
--Sales-Transaction Log Backup	           NULL  	2
--Sales-Differential Database backup	   NULL  	5

-- INSERT ANOTHER DATA
Insert Into Products Values ('Phone', 'Apple')


-- TAKING T.LOG BACKUP
Backup Log Sales
To Disk = N'I:\MSSQL\Sales.bak'
With 
Name = N'Sales-Transaction Log backup',
STATS = 10
Go

-- VIEW DATA
Select * From Products  -- 13 Records

-- LOOK INSIDE THE .bak file
Restore HeaderOnly From Disk = N'I:\MSSQL\Sales.bak' -- 1 FULL BACKUP, 5 T.LOG, 2 DIFFERENTIAL BACKUP

--OUTPUT BELOW
    --BackupName                                -- BackupType
-- 1 sales-Full Database Backup	               NULL  	1
-- 2 sales-Transaction Log  Backup	           NULL	    2
-- 3 sales-Transaction Log  Backup	           NULL	    2
-- 4 sales-Differential Database  Backup	   NULL  	5
-- 5 Sales-Transaction Log Backup	           NULL  	2
-- 6 Sales-Transaction Log Backup	           NULL  	2
-- 7 Sales-Differential Database backup	       NULL  	5
-- 8 Sales-Transaction Log backup	           NULL	    2

-- IN RESTORING DATABASE FROM BACKUPS
-- RESTORING => Restore from the first FULL BACKUP and the last DIFFERENTIAL BACKUP so skip all other T.LOG and DIFFERNTIAL that has been taken between the First Full backup and the last DIfferential backup



-- LETS VALID THAT => VERIFY THAT THE DATABASE BACKUP IS VALID (not that the data within is valid)
declare @backupSetId as int
select @backupSetId = position
from msdb..backupset
where database_name=N'sales'
and backup_set_id=(select max(backup_set_id)
from msdb..backupset where database_name=N'sales' )
if @backupSetId is null
begin
raiserror(N'Verify failed. Backup information for database ''sales'' not found.', 16, 1)
end 
RESTORE VERIFYONLY
FROM  DISK = N'I:\MSSQL\sales.bak'
WITH  FILE = @backupSetId
GO
 
-- TRANSACTION LOG BACKUP
   -- You must backup the transaction log, if SQL Server database uses either FULL or BULK-LOGGED recovery model otherwise transaction log is going to full. Backing up the transaction log truncates the log and user should be able to restore the database to a specific point in time.




-- 31 INTRODUCTION TO RESTORE  
------------------------------------------------------------------------------------------------


-- RESTORING A DATABASE

-- The principal reason we take backups of system and user defined databases is that it allows us to restore the databases in case of a disaster. There are a few restore commands that we should be familiar with and they are as follows:

-- RESTORE COMMANDS:
   -- • RESTORE DATABASE - allows us to restore a full, differential, file or filegroup backup
   -- • RESTORE LOG - allows us to restore a transaction log backup

-- INFORMATIONAL RESTORE COMMANDS:
   -- • RESTORE HEADERONLY - gives you a list of all the backups in a file
   -- • RESTORE FILELISTONLY - gives you a list of all of the files that were backed up for a give backup
   -- • RESTORE VERIFYONLY - verifies that the backup is readable by the RESTORE process
   -- • RESTORE LABELONLY - gives you the backup media information

-- RESTORE HEADERONLY
   -- The RESTORE HEADERONLY option allows you to see the backup header information for all backups for a particular backup device.

-- EXAMPLE:
RESTORE HEADERONLY FROM DISK = 'C:\FullBackups\test.bak'
GO

-- RESTORE FILELISTONLY
   -- The RESTORE FILELISTONLY option allows you to see a list of the files that were backed up such as the .mdf and .ldf.

-- EXAMPLE:
RESTORE FILELISTONLY FROM DISK = 'C:\FullBackups\test.bak'
GO

-- RESTORE VERIFYONLY
   -- Verifies that the backup is readable by the RESTORE process

-- EXAMPLE:
RESTORE VERIFYONLY from disk = 'C:\FullBackups\test.bak'

-- RESTORE LABELONLY
   -- The RESTORE LABELONLY option allows you to see the backup media information for the backup device




-- 32 RESTORE DATABASE WITH GUI
----------------------------------------------------------------------------------------------------------------------------

-- DROP DATABASE SALES
	--Use master
	--Drop database Sales

Use master
Exec msdb.dbo.sp_delete_database_backuphistory @database_name = N'Sales'

Use master
Alter Database Sales Set Single_User With Rollback Immediate

Use master
Drop Database Sales

-- CREATE DATABASE Sales 
Use Master
Create Database Sales
Go


Use Sales
Go

-- CREATE TABLE
Create Table Products
(ProductID int Identity(1,1) Primary Key,
ProductName varchar(100),
Brand varchar(100))

-- INSERT DATA INTO PRODUCTS TABLE
insert into Products values ('Bike','Genesis')
insert into Products values ('Hat','Nike')
insert into Products values ('Shoe','Payless')
insert into Products values ('Phone','Apple')
insert into Products values ('Book','Green')
insert into Products values ('Cup','Large')

-- VIEW DATA
Select * From Products -- 6 records present

-- TAKE FULL DATABASE BACKUP OF THE SIX ROWS DATA INSERTED
BACKUP DATABASE Sales
TO  DISK = N'I:\MSSQL\sales.bak'
WITH  NOINIT, -- No Override
NAME = N'sales-Full Database Backup',
STATS = 10
GO

-- LOOK INSIDE THE .bak file
Restore HeaderOnly From Disk = N'I:\MSSQL\Sales.bak' -- Six records present

-- INSERT ONE MORE ROW DATA INTO THE TABLLE AFTER A FULL DATABASE BACKUP, BUT BEFORE A TRANSACTION LOG BACKUP
insert into Products values ('Doll','Toy_R_us')

-- VIEW DATA
Select * From Products  -- 7 records present

-- TAKING A TRANSACTION LOG BACKUP (this can only be taken if the database recovery model is in FULL MODE, and a FULL database backup had been executed)
BACKUP LOG [sales]
TO  DISK = N'I:\MSSQL\sales.bak'
WITH
NAME = N'sales-Transaction Log  Backup',
STATS = 10
GO

-- LOOK INSIDE THE .bak file
Restore HeaderOnly From Disk = N'I:\MSSQL\Sales.bak' -- 1 Time FULL and 1 Time T.LOG (2 RECORDS)

-- INSERT ANOTHER DATA
Insert into Products values ('House', 'Remax')

-- VIEW DATA
Select * From Products -- 8 records

-- TAKING ANOTHER TRANSACTION LOG BACKUP
BACKUP LOG [sales]
TO  DISK = N'I:\MSSQL\sales.bak'
WITH
NAME = N'sales-Transaction Log  Backup',
STATS = 10
GO

-- LOOK INSIDE THE .bak file
Restore HeaderOnly From Disk = N'I:\MSSQL\Sales.bak' -- 1 Time FULL, 5 Time T.LOG and 2 time DIFFERENTIAL

-- At this point we have 1 Full database backup with data from 1 to 6 rows
-- 1 first T.LOG backup of row 7
-- 2 first T.LOG backup of row 8

-- To restore the database if a disaster occured, we will need ALL the FULL, and the 2 TRANSACTIONAL LOGS backup to recover all the data from 1 through 8 rows

-- Lets DELETE the database SALES and RESTORE using GUI
-- NOTE! If you only have 1 time FULL BACKUP and 2 or more T.LOGS backup when RESTORING you can restore to time or period you want means you can skip the last T.LOG BACKUP to restore the database to particular time BUT you can not skip in the middle of the sequence its like a chain so you need to restore all the T.LOGS backup files that is also for you to get all the data back online



-- 33 RESTORE DATABASE USING DIFFERENTIAL 
--------------------------------------------------------------------------------------------------------------------------

-- If you have multiple differential backup eg 1234, the 4 one will have all the changes from 123 
-- RESTORE using DIFFERENTIAL we do not to do it sequence like T.LOG. So we need first FULL BACKUP, last DIFFERENTIAL backup and the rest T.LOGS backup

-- LOOK INSIDE THE .bak file
Restore HeaderOnly From Disk = N'I:\MSSQL\Sales.bak' -- 1 Time FULL, 5 Time T.LOG and 2 time DIFFERENTIAL

--OUTPUT BELOW
    --BackupName                                -- BackupType
-- 1 sales-Full Database Backup	               NULL  	1
-- 2 sales-Transaction Log  Backup	           NULL	    2
-- 3 sales-Transaction Log  Backup	           NULL	    2
-- 4 sales-Differential Database  Backup	   NULL  	5
-- 5 Sales-Transaction Log Backup	           NULL  	2
-- 6 Sales-Transaction Log Backup	           NULL  	2
-- 7 Sales-Differential Database backup	       NULL  	5
-- 8 Sales-Transaction Log backup	           NULL	    2

-- IN RESTORING DATABASE FROM BACKUPS
-- RESTORING => Restore from the first FULL BACKUP and the last DIFFERENTIAL BACKUP so skip all other T.LOG and DIFFERNTIAL that has been taken between the First Full backup and the last DIfferential backup and take also the rest T.LOGS after the last DIFFERENTIAL backup.

-- TAKE A LOOK HERE 
-- 1 sales-Full Database Backup	               NULL  	1
-- 7 Sales-Differential Database backup	       NULL  	5
-- 8 Sales-Transaction Log backup	           NULL	    2

-- RESTORE USING GUI
-- Right click on the DATABASES - RESTORE DATABASE - DEVICE - ECLIPSE BTN - ADD - Select the BACKUPS file from the drive folder - Ok - Ok - Automatically select 1 first FULL BACKUP, 2 select last DIFFERENTIAL and the rest T.LOGS backups - OK - Done


-- VIEW DATA
Use Sales
Select * From Products -- 10 records present



-- 34 RECOVERY MODE WITH TAIL LOG BACKUP
-----------------------------------------------------------------------------------------------

-- RECOVERY STATE => When you are trying to restore database - Right click on databaseName - Task - Restore - Database -New window open - Click Option - Recovery State - there is the 3 RECOVERY STATE

   -- There are 3 RECOVERY STATE
      -- 1. RESTORE WITH RECOVERY => Restore database from Full backup and their are no more other file to restore such as T.LOG or Differential
	  -- 2. RESTORE WITH NORECOVERY => Means you have more files to recover such as T.LOG files means ADDITIONAL TRANSACTION LOGS CAN BE RESTORED. 
	  -- 3. RESTORE WITH STANDBY => 


-- IN PRACTICAL
------------------------------------
-- DROP DATABASE SALES
	--Use master
	--Drop database Sales

EXEC msdb.dbo.sp_delete_database_backuphistory @database_name = N'Sales'
GO
use [master];
GO
USE [master]
GO
ALTER DATABASE [Sales] SET  SINGLE_USER WITH ROLLBACK IMMEDIATE
GO
USE [master]
GO
/****** Object:  Database [Sales]    Script Date: 7/22/2023 12:25:12 AM ******/
DROP DATABASE [Sales]
GO



-- CREATE DATABASE Sales 
Use Master
Create Database Sales
Go


Use Sales
Go

-- CREATE TABLE
Create Table Products
(ProductID int Identity(1,1) Primary Key,
ProductName varchar(100),
Brand varchar(100))

-- INSERT DATA INTO PRODUCTS TABLE
insert into Products values ('Bike','Genesis')
insert into Products values ('Hat','Nike')
insert into Products values ('Shoe','Payless')
insert into Products values ('Phone','Apple')
insert into Products values ('Book','Green')
insert into Products values ('Cup','Large')

-- VIEW DATA
Select * From Products -- 6 records present

-- 1 TAKE FULL DATABASE BACKUP OF THE SIX ROWS DATA INSERTED
BACKUP DATABASE Sales
TO  DISK = N'I:\MSSQL\sales.bak'
WITH  NOINIT, -- No Override
NAME = N'sales-Full Database Backup',
STATS = 10
GO


-- 1. INSERT ONE MORE ROW DATA INTO THE TABLLE AFTER A FULL DATABASE BACKUP, BUT BEFORE A TRANSACTION LOG BACKUP
insert into Products values ('Doll','Toy_R_us')

-- VIEW DATA
Select * From Products -- 7 records present

-- 2 TAKING A TRANSACTION LOG BACKUP (this can only be taken if the database recovery model is in FULL MODE, and a FULL database backup had been executed)
BACKUP LOG [sales]
TO DISK = N'I:\MSSQL\sales.bak'
WITH
NAME = N'sales-Transaction Log  Backup',
STATS = 10
GO


-- 2. INSERT ANOTHER DATA
Insert into Products values ('House', 'Remax')

-- VIEW DATA
Select * From Products -- 8 records present

-- 3 TAKING A TRANSACTION LOG BACKUP (this can only be taken if the database recovery model is in FULL MODE, and a FULL database backup had been executed)
BACKUP LOG [sales]
TO  DISK = N'I:\MSSQL\sales.bak'
WITH
NAME = N'sales-Transaction Log  Backup',
STATS = 10
GO


-- 3. INSERT ANOTHER DATA
Insert into Products values ('Car', 'Porche')

-- VIEW DATA
Select * From Products -- 9 records present

-- 4 TAKING DIFFERENTIAL BACKUP
BACKUP Database [sales]
TO  DISK = N'I:\MSSQL\sales.bak'
WITH DIFFERENTIAL,
NOINIT, -- No override
NAME = N'sales-Differential Database Backup',
STATS = 10
GO


-- 4. INSERT ANOTHER DATA
Insert into Products values ('Chair', 'Walmat')

-- VIEW DATA
Select * From Products -- 10 records present

-- 5 TAKING ANOTHER TRANSACTION LOG BACKUP
BACKUP LOG [sales]
TO  DISK = N'I:\MSSQL\sales.bak'
WITH 
NAME = N'sales-Transaction Log Backup',
STATS = 10
GO



-- 5. INSERT ANOTHER DATA
Insert into Products values ('Mouse', 'Apple')

-- VIEW DATA
Select * From Products -- 11 records present

-- 6 TAKING ANOTHER TRANSACTION LOG BACKUP
BACKUP LOG [sales]
TO  DISK = N'I:\MSSQL\sales.bak'
WITH 
NAME = N'sales-Transaction Log Backup',
STATS = 10
GO


-- 6. INSERT ANOTHER DATA
Insert into Products values ('TV', 'Sony')

-- VIEW DATA
Select * From Products -- 12 records present

-- 7 TAKING DIFFERENTIAL BACKUP
BACKUP Database [sales]
TO  DISK = N'I:\MSSQL\sales.bak'
WITH DIFFERENTIAL,
NOINIT, -- No override
NAME = N'sales-Differential Database Backup',
STATS = 10
GO


-- 7. INSERT ANOTHER DATA
Insert into Products values ('Phone', 'Apple')

-- VIEW DATA
Select * From Products -- 13 records present

-- 8 TAKING ANOTHER TRANSACTION LOG BACKUP
BACKUP LOG [sales]
TO DISK = N'I:\MSSQL\sales.bak'
WITH 
NAME = N'sales-Transaction Log Backup',
STATS = 10
GO


-- LOOK  INSIDE THE .bak file
Restore HeaderOnly From Disk = N'I:\MSSQL\Sales.bak'


-- VIEW THE DATA => 13 Rows
Select * From Products



-- INSERTS HAPPENED HERE BUT NOT BACKUP
Insert into Products Values ('DESK', 'IKEA')
Insert into Products Values ('LAMP', 'SUMMERS')
Insert into Products Values ('PENS', 'BIGGS')
Insert into Products Values ('PILLOW', 'SAMS')

-- VIEW THE DATA => 17 Rows
Select * From Products



-- DATABASE IS CORRUPT! NEED TO RESTORE ASAP (HOW DO YOU GET THE LAST INSERTS)
   -- TAKE A TAIL LOG IF POSSIBLE
   
-- AT THIS POINT, IF I WERE TO RESTORE THE DATABASE WITH POSITION 1, 7 AND 8, I WOULD RETRIVE 13 ROWS, NOT 17. THUS I NEED TO TAKE TAIL LOG BACKUP IF POSSIBLE 

-- LOOK  INSIDE THE .bak file
Restore HeaderOnly From Disk = N'I:\MSSQL\Sales.bak'  


-- BEFORE RESTORING THE DATABASE, TAKE A LAST TAIL-LOG BACKUP IF POSSIBLE WITH NORECOVERY OPTION TO GRAB THE LAST FOUR INSERTS 

Use Master
Go

-- TAKING ANOTHER TRANSACTION LOG BACKUP

BACKUP LOG [sales]
TO  DISK = N'I:\MSSQL\sales.bak'
WITH NO_TRUNCATE, -- must have the NO_TRUNCATE option set, so as not to delete the remaining four inserts
COPY_ONLY,        -- also, have the option COPY_ONLY  so as to break the chain of sequential T-LOGS of the backup set
NOFORMAT, NOINIT,
NAME = N'sales-Transaction Log Backup',
SKIP, NOREWIND, NOUNLOAD,
NORECOVERY,       -- this NORECOVERY option puts the database into RESTORING MODE 
STATS = 10
GO

-- IN CASE A QUERY IS STILL RUNNING OR IN USE, YOU CAN JUST KILL THE SESSION FOR EXAMPLE SESSION 54 IS IN USED 
   sp_who  -- ==> TO SEE WHAT SESSION IS RUNNING 
   kill 57 -- ==> KILL THE PROCESS



-- THEN RESTORE THE DATABASE USING THE FOLLOWING SCRIPT IN ORDER (FULL, LAST DIFFERENTIAL, ALL T-LOGS)
-- FULL BACKUP
USE [master]
Go


RESTORE DATABASE Sales
FROM DISK = N'I:\MSSQL\Sales.bak'
WITH FILE = 1,
NORECOVERY, 
NOUNLOAD, 
STATS = 5

-- LAST DIFFERENTIAL
RESTORE DATABASE Sales
FROM DISK = N'I:\MSSQL\Sales.bak'
WITH FILE = 7,
NORECOVERY, 
NOUNLOAD, 
STATS = 5


-- T-LOG
RESTORE LOG Sales
FROM DISK = N'I:\MSSQL\Sales.bak'
WITH FILE = 8,
NORECOVERY,
NOUNLOAD,  
STATS = 5
Go


-- TAIL LOG RESTORE
RESTORE LOG Sales
FROM DISK = N'I:\MSSQL\Sales.bak'
WITH FILE = 9,
RECOVERY,
NOUNLOAD,  
STATS = 5
Go

-- VIEW THE DATA => 17 Rows
Use Sales
Select * From Products



-- 35. BACKUP USING MAINTENANCE PLAN
---------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------

-- Management - Right Click on Management - Maintenance Plan Wizard - Next - Name - System Backups -Run as: - SQL Server Agent service account - make sure SQL SERVER AGENT is running if is not TURN it on - from SQL SERVER CONFIGURATION MANAGER - Select - Single schedule for the entire plan or no schedule - Schedule - Not schedule (On Demand) - Next - Select Maintenance Tasks (Which tasks should this plan perform?) - Select - Back Up Database(Full) - Next - Next - Databases - Select one or more - System Databases - Select - Ignore Database that is not online - Ok - Backup set will expire - Dont select - Selecct - Create a backup file for every database - But not Create a Sub-directory for each database - Select - Verify backup integrity - Next - Select - Write a report to a text file - Next - Finish

-- NOW! - There is SYSTEM BACKUPS under MAINTENANCE PLANS - Right click on the SYSTEM BACKUPS - View Histories - But there is no history because w have not schedule any plan

-- SYSTEM BACKUPS - Modify - Subplan - Schedule - Name - System Backups Subplan_1 - (Sytem Backups) - Schedule type - Recuring - Enable - Frequency - Daily - Occurs every - 1 hour - Starting at - 8:40pm -Ending at - 8:41pm - Ok - Once you see at the end of the name System Bcakups [Design]* - it means its not been backup - Click SAVE to saved it. 

-- TO VERIFY IF THE JOB HAS RUNNING - SQL Server Agent - Jobs - Right click on the - Sytem Backups.Sytem Backups - there you will the logs




-- 36. BACKUP DATABASDE USING MAINTENANCE PLAN
-----------------------------------------------------------------------------------------------------------------------------------------
Use Master
Create Database Admin
Go

-- USING SUBPLAN
   -- FULL DATABASE BACKUP
   -- DIFFERENTIAL BACKUP
   -- TRANSACTION BACKUP

-- Right click on the Maintenance Plans - New Maintenance Plan - Name - ADMIN FULL BACKUP - Double Click on the - Subplan_1 - Name - FULL - Description - Delete the Subplan_1 - Ok - Add Subplan - Name - DIFF - Description - Delete the DIFF - Ok - Add Subplan - Name - TLOG - Description - Delete the TLOG - Ok - 

-- LET MAKE SUBPLAN

   -- Click on the FULL and bring the BACKUP DATABASE TASK down in the middle - Double click it -  Backup Type - FULL - Chose Database - Admin - Ignore database where the state is not online - Ok 


   -- Click on the DIFF and bring the BACKUP DATABASE TASK down in the middle - Double click it - Backup Type - DIFFERENTIAL - Chose Database - Admin - Ignore database where the state is not online - Backup file extension - DIFF - Ok 


   -- Click on the TLOG and bring the BACKUP DATABASE TASK down in the middle - Double click it - Backup Type - TRANSACTION LOG - Chose Database - Admin - Ignore database where the state is not online - Backup file extension - trn - Ok 


-- LETS SCHEDULE THEM 
  
   -- FULL  => Schedule type - Recurring - Enable - Schedule - Occurs - Daily - Occurs once at - 9:45 pm - No End Date - Ok
   -- DIFF  => Schedule type - Recurring - Enable - Schedule - Occurs - Daily - Occurs every - 2 minutes - Starting at - 9:46pm - Ending at - 9:50:59pm - No End Date - Ok
   -- TLOG  => Schedule type - Recurring - Enable - Schedule - Occurs - Daily - Occurs every - 10 seconds - Starting at - 9:45pm - Ending at - 9:50:59pm - No End Date - Ok

   -- Save it

-- EXECUTE the following script
USE Admin
GO

CREATE TABLE dbo.WHILE_TABLE 
(
FIRSTNAME VARCHAR (800)
)
GO
  

-- USING A WHILE LOOP COMMAND TO POPULATE DATA INTO A TABLE

declare @i int;
SET @i = 0;
while @i < 100
begin
	INSERT INTO DBO.WHILE_TABLE (FIRSTNAME)
	VALUES(cast(replicate('DBA',203) AS VARCHAR (20)));
	SET @i = @i + 1;
end;
GO

WAITFOR DELAY '00:00:05' --<<USING THE WAITFOR DELAY TO 'PAUSE' THE EXECUTIONOF THE NEXT COMMAND

-- USING A WHILE LOOP COMMAND TO POPULATE DATA INTO A TABLE

declare @i int;
SET @i = 0;
while @i < 100
begin
	INSERT INTO DBO.WHILE_TABLE (FIRSTNAME)
	VALUES(cast(replicate('DBA',203) AS VARCHAR (20)));
	SET @i = @i + 1;
end;
GO

-- USING A WHILE LOOP COMMAND TO POPULATE DATA INTO A TABLE

WAITFOR DELAY '00:00:10'

declare @i int;
SET @i = 0;
while @i < 100
begin
	INSERT INTO DBO.WHILE_TABLE (FIRSTNAME)
	VALUES(cast(replicate('DBA',203) AS VARCHAR (20)));
	SET @i = @i + 1;
end;
GO

-- USING A WHILE LOOP COMMAND TO POPULATE DATA INTO A TABLE

WAITFOR DELAY '00:00:15'

declare @i int;
SET @i = 0;
while @i < 100
begin
	INSERT INTO DBO.WHILE_TABLE (FIRSTNAME)
	VALUES(cast(replicate('DBA',203) AS VARCHAR (20)));
	SET @i = @i + 1;
end;
GO

-- USING A WHILE LOOP COMMAND TO POPULATE DATA INTO A TABLE

WAITFOR DELAY '00:00:10'

declare @i int;
SET @i = 0;
while @i < 100
begin
	INSERT INTO DBO.WHILE_TABLE (FIRSTNAME)
	VALUES(cast(replicate('DBA',203) AS VARCHAR (20)));
	SET @i = @i + 1;
end;
GO

-- USING A WHILE LOOP COMMAND TO POPULATE DATA INTO A TABLE

declare @i int;
SET @i = 0;
while @i < 100
begin
	INSERT INTO DBO.WHILE_TABLE (FIRSTNAME)
	VALUES(cast(replicate('DBA',203) AS VARCHAR (20)));
	SET @i = @i + 1;
end;
GO

WAITFOR DELAY '00:00:05' -- <<USING THE WAITFOR DELAY TO 'PAUSE' THE EXECUTIONOF THE NEXT COMMAND

-- USING A WHILE LOOP COMMAND TO POPULATE DATA INTO A TABLE

declare @i int;
SET @i = 0;
while @i < 100
begin
	INSERT INTO DBO.WHILE_TABLE (FIRSTNAME)
	VALUES(cast(replicate('DBA',203) AS VARCHAR (20)));
	SET @i = @i + 1;
end;
GO

-- USING A WHILE LOOP COMMAND TO POPULATE DATA INTO A TABLE

WAITFOR DELAY '00:00:10'

declare @i int;
SET @i = 0;
while @i < 100
begin
	INSERT INTO DBO.WHILE_TABLE (FIRSTNAME)
	VALUES(cast(replicate('DBA',203) AS VARCHAR (20)));
	SET @i = @i + 1;
end;
GO

-- USING A WHILE LOOP COMMAND TO POPULATE DATA INTO A TABLE

WAITFOR DELAY '00:00:15'

declare @i int;
SET @i = 0;
while @i < 100
begin
	INSERT INTO DBO.WHILE_TABLE (FIRSTNAME)
	VALUES(cast(replicate('DBA',203) AS VARCHAR (20)));
	SET @i = @i + 1;
end;
GO

-- USING A WHILE LOOP COMMAND TO POPULATE DATA INTO A TABLE

WAITFOR DELAY '00:00:10'

declare @i int;
SET @i = 0;
while @i < 100
begin
	INSERT INTO DBO.WHILE_TABLE (FIRSTNAME)
	VALUES(cast(replicate('DBA',203) AS VARCHAR (20)));
	SET @i = @i + 1;
end;
GO

SELECT * FROM DBO.WHILE_TABLE

-- 1000 ROWS





-- 37. MAINTENANCE PLAN TASKS
-----------------------------------------------------------------------------------------------------------------------

-- CHECK DATABASE INTEGRITY PURPOSE:
   -- •	The DBCC CHECKDB performs an internal consistency check and should be done off hour
   -- • very resource intensive
   -- •	perform it on a regular basis

-- SHRINK DATABASE
   -- •	Shrinking a database is a very poor practice in the SQL world.  Don’t do it. It causes perfomance issue

-- REBUILD INDEX
   -- •	The Rebuild Index task physically rebuilding indexes from scratch
   -- •	This removes index fragmentation and updates statistics at the same time


-- REORGANIZE INDEX
   -- •	The Reorganize Index task helps to remove index fragmentation, but does not update index and column statistics
   -- •	If you use this option to remove index fragmentation, then you will also need to run the Update Statistics task as part of the same Maintenance Plan


-- UPDATE STATISTICS
   -- •	The Update Statistics task runs the sp_updatestats system stored procedure against the tables 
   -- •	Don't run it after running the Rebuild Index task, as the Rebuild Index task performs this same task automatically


-- EXECUTE SQL SERVER AGENT JOB
   -- •	The Execute SQL Server Agent Job task allows you to select SQL Server Agent jobs (ones you have previously created), and to execute them as part of a Maintenance Plan


-- HISTORY CLEANUP => Very important
   -- •	The History Cleanup task deletes historical data from the msdb database 
   -- •	backup and restore
   -- •	SQL Server Agent and Maintenance Plans



-- BACK UP DATABASE (FULL)
   -- •	The Back Up Database (Full) task executes the BACKUP DATABASE statement and creates a full backup of the database


-- BACK UP DATABASE (DIFFERENTIAL)
   -- •	The Back Up Database (Differential) task executes the BACKUP DATABASE statement using the DIFFERENTIAL option


-- BACKUP DATABASE (TRANSACTION LOG)
   -- •	The Backup Database (Transaction Log) task executes the BACKUP LOG statement, and, in most cases, should be part of any Maintenance Plan that uses the Back Up Database (Full) task


-- MAINTENANCE CLEANUP TASK
   -- •	Use a batch file to clean up files




-- 38. DON'T SHRINK A DATABASE
----------------------------------------------------------------------------------

-- SHRINKING A DATABASE
   -- The primary purpose of shrinking a database (or individual files) is to reclaim the SPACE by removing unused pages when a file no longer needs to be as large as it once was; shrinking the file may then become necessary, but as we will demonstrate, this process is highly discouraged and is an extremely poor practice in the production database environment.

   -- 100mb mdf files         1000mb mdf files           100mb


-- THINGS TO NOTE
   -- TSQL => DBCC SHRINKDATABASE
   -- •	Both data and transaction log files can be shrunk individually or collectively
   -- •	When using the DBCC SHRINKDATABASE statement, you cannot shrink a whole database to be smaller than its original size. However, you can shrink the individual database files to a smaller size than their initial size by using the DBCC SHRINKFILE statement.
   -- •	The size of the virtual log files within the log determines the possible reductions in size. 
   -- •	Shrinking causes massive fragmentation and will just result in the data file growing again next time data gets added.  When that happens, the entire system will slow down as the file is expanded. 

-- AUTOMATIC DATABASE SHRINKING
   -- When the AUTO_SHRINK database option has been set to ON, the Database Engine automatically shrinks databases that have free space. By default, it is set to OFF. Leave it as is.

-- DATABASE SHRINKING COMMANDS
   -- •	DBCC SHRINKDATABASE (ShrinkDB, 10)
   -- •	DBCC SHRINKFILE (ShrinkDB, 10)

-- BEST PRACTICES
   -- •	Size the database accordingly so as to prevent shrinking and expanding the database
   -- •	When unused space is created due to dropping of tables, shrinking of the database may be needed - but rarely
   -- •	Don't constantly shrink the database as it will grow again
   -- •	When you shrink a database, the indexes on tables are not preserved and as such will causes massive fragmentation and will have to be rebuilt
   -- •	Shrinking the log file may be necessary if your log has grown out of control however, shrinking the log should be rare also
   -- •	Regular maintenance plans should never have shrink database job
   -- •	Other issues that shrinking causes are lot of I/O, of CPU usage, and transaction log gets larger – as everything it does is fully logged.

/*the following example will illustrate that shrinking a database will

1. Causes pages being moved from the front of file to the end
2. It will cause massive fragmentation
3. It will not reduce the size of the data file, but actually increase it
4. That shrinking a database will cause unnecessary I/O hits
5. That shrinking a database was a futile endeavor because of poor planning regarding sizing of database
6. And that shrinking a database should be very rarely done
*/

USE [master];
GO
 
CREATE DATABASE ShrinkDB;
GO

USE [ShrinkDB];
GO
 

-- CREATE AN INITIAL TABLE AT THE 'FRONT' OF THE DATA FILE
CREATE TABLE Initial (
    [col1] INT IDENTITY,
    [col2] CHAR (8000) DEFAULT 'Front');
GO


-- INSERT DATA INTO INITIAL TABLE
INSERT INTO Initial DEFAULT VALUES;
GO 1500 -- 1500 times

select * from Initial

-- CHECK THE SIZE OF THE DATABASE
sp_helpdb [ShrinkDB] -- 14.83 MB
 
-- CREATE THE SECOND TABLE, WHICH WILL BE CREATED 'AFTER' THE INITIAL TABLE IN THE DATA FILE
CREATE TABLE second (
    [col1] INT IDENTITY,
    [col2] CHAR (8000) DEFAULT 'after');

-- CREATE A CLUSTERD INDEX ON THE SECOND TABLE
CREATE CLUSTERED INDEX [coll] ON second ([col1]);
GO
 

-- INSERT DATA INTO SECOND TABLE
INSERT INTO second DEFAULT VALUES;
GO 1500


select * from second

-- CHECK DB SIZE
sp_helpdb [ShrinkDB] -- 26.83 MB

-- Database expanded due to insert of data in the second table (26.83 MB)
 
-- CHECK THE FRAGMENTATION OF THE SECOND TABLE
SELECT
    [avg_fragmentation_in_percent]
FROM sys.dm_db_index_physical_stats (
    DB_ID (N'ShrinkDB'), OBJECT_ID (N'second'), 1, NULL, 'LIMITED');
GO

-- Output -- 0.4 avg_fragmentation_in_percent


-- Notice that the fragmentation of the clustered index for the second table is almost zero before the shrink

-- We will now drop the initial table we created and execute the shrink command to reclaim the SPACE at the front of the data file
-- then see what happens to the fragmentaion.

DROP TABLE Initial;
GO


sp_helpdb [ShrinkDB]
-- 26.83 MB the data file has not shrunk due to the deletion of the initial table

-- Even after deleting INITIAL TABLE still no changes in the size it suppose to be free more space

-- SHRINK THE DATABASE
DBCC SHRINKDATABASE ([ShrinkDB]);
GO

-- Notice that the SPACE after the shrink went down from 26.83 to 15.02 mb
sp_helpdb [ShrinkDB]
-- 15.02 MB -- 14.77 MB -- It did shrink the database
 
-- But notice what happened to the fragmentation of the data file because of the shrinking of the database???
-- When Checking the index fragmentation again, we notice that the fragmentation has drastically increased to almost 100%!!!
-- This is because we have shuffled all the data pages and the index is not in a sorted position

SELECT
    [avg_fragmentation_in_percent]
FROM sys.dm_db_index_physical_stats (
    DB_ID (N'ShrinkDB'), OBJECT_ID (N'second'), 1, NULL, 'LIMITED');
GO

-- Output -- 99.6 avg_fragmentation_in_percent

-- While the database has shrunk, and we have reclaimed space from the data file, we MUST now the fix the fragmented index of the table by rebuilding the index!!!


-- REBUILD THE CLUSTERED INDEX -- To increase the performance
ALTER INDEX [coll] ON second REBUILD
GO


-- Checking the index fragmentation again indicates that the fragmentaion of the index has been restored, but notice the size of the data
-- file when we run the sp_helpdb [ShrinkDB] - it has actually GROWN even more than it started from!!!

SELECT
    [avg_fragmentation_in_percent]
FROM sys.dm_db_index_physical_stats (
    DB_ID (N'ShrinkDB'), OBJECT_ID (N'second'), 1, NULL, 'LIMITED');
GO

-- Output -- 0.2% avg_fragmentation_in_percent

sp_helpdb [ShrinkDB]
-- 41.81 MB -- It has increase in size WHY because rebuilding of the index and the logging of the index

-- The database file has grown because of the rebuilding of the index and the logging of the index

-- Use master
-- Go
-- select * from sys.databases

-- sp_helpdb ShrinkDB
-- transaction log = 13888 KB

-- INDIVIDUAL FILE SHRINK
USE ShrinkDB
GO
DBCC SHRINKFILE (N'ShrinkDB_Log' , 0, TRUNCATEONLY)
GO

sp_helpdb ShrinkDB

-- 784 KB



-- USE ShrinkDB
-- GO
-- DBCC SHRINKFILE (N'ShrinkDB_Data' , 0, TRUNCATEONLY)
-- GO


-- USE ShrinkDB
-- GO
-- DBCC SHRINKDATABASE(N'ShrinkDB' )
-- GO



-- 39. THE IMPORTANCE OF USING SQL SERVER AGENT
-------------------------------------------------------------------------------------------------------------

-- WHAT IS THE SQL SERVER AGENT?
	-- • SQL Server Agent uses SQL Server to store job information
	-- • A job is a specified series of actions that can run on a local server or on multiple remote servers
	-- • Jobs can contain one or more other job steps
	-- • SQL Server Agent can run on a job on a schedule
	-- • To a response or a specific event or manually
	-- • You can avoid repetitive tasks by automating those tasks
	-- • SQL Server Agent can record the event and notify you via email or pager
	-- • You can set up a job using the SQL Agent for SSIS packages or for Analysis Services 
	-- • More than one job can run on the same schedule, and more than one schedule can apply to the same job

-- ALERTS
	-- • An alert is an automatic response to a specific event. Which you can you define the conditions under which an alert occurs fires
	
--OPERATORS
	-- • An operator is the contact person for alerts via the following: 
	-- • E-mail
	-- • Pager (through e-mail)
	-- • net send

-- SECURITY FOR SQL SERVER AGENT ADMINISTRATION
	-- • SQL Agent uses the following msdb database roles to manage security:
	-- • SQL Agent User Role
	-- • SQL Agent Reader Role
	-- • SQL Agent Operator Role 

-- SQL SERVER AGENT MULTI SERVER ADMINISTRATION

-- Using Master/Target servers to manage many jobs at once

-- Error Logs and job activity monitor




-- 40. HOW TO CREATE A SIMPLE BACKUP JOB USING SQL SERVER AGENT
--------------------------------------------------------------------------------------------------------------------------------------
Use Master
Create Database Admin


BACKUP Database [Admin]
TO  DISK = N'I:\MSSQL\Admin.bak'
WITH
NOINIT,
NAME = N'Admin-Full Database Backup',
STATS = 10
GO



-- STEPS -- Right click on the Jobs - Click New Job - Name Full Admin Backup - Enable (if is not enable) - Click Steps - Click New - General - Name Full Admin Backup - Type - Transact-SQL script /T-SQL) - Database - master - Command - copy the script inside the box - 
BACKUP Database [Admin]
TO  DISK = N'I:\MSSQL\Admin.bak'
WITH
NOINIT,
NAME = N'Admin-Full Database Backup',
STATS = 10
GO 
-- Parse (Parse the command text) - Ok - Advance - On success action - Go to the next step - On failure action - Quit the job reporting failure - Ok 
-- Schedules - New - Name - Full Admin Schedule - Schedule Type - Recurring - Enable - Frequency - Daily - Recurs - 1 Days - Daily Frequency - Occurs once at: 9:30pm - Ok - Ok
-- If you open Jobs - Local Jobs - You will see - Full Admin Backup



-- 41. THE IMPORTANCE OF DBCC CheckDB
---------------------------------------------------------------------------------------------------

DBCC CHECKDB
-- WHAT IS THE PURPOSE OF DBCC CHECKDB
   -- The primary purpose is to check both the logical and the physical integrity of all objects in the specified database. In a busy and large production database, it may become necessary to run a few selected options that the DBCC CHECKDB provides.

-- COMPLETE SYNTAX OF DBCC CHECKDB
DBCC CHECKDB 
    ( 'database_name' 
            [, NOINDEX 
                | { REPAIR_ALLOW_DATA_LOSS 
                    | REPAIR_FAST 
                    | REPAIR_REBUILD 
                    } ] 
    )    [ WITH { [ ALL_ERRORMSGS ] 
                    [ , [ NO_INFOMSGS ] ] 
                    [ , [ TABLOCK ] ] 
                    [ , [ ESTIMATEONLY ] ] 
                    [ , [ PHYSICAL_ONLY ] ] 
                    } 
        ] 



-- DBCC CHECKDB SYNTAX OPTIONS

DBCC CHECKDB ('adventureworks2019') 
-- In very large databases (VLDB) this may not be an option as it's resource intense 
	-- But should be done on a prod database, to get a base line metric
	-- Output - Messages - CHECKDB found 0 allocation errors and 0 consistency errors in database 'AdventureWorks2019'.
	-- Checks all data in database -- 7 seconds


-- SPECIFIES THAT NON CLUSTERED INDEXES FOR NON SYSTEM TABLES SHOULD NOT BE CHECKED
DBCC CHECKDB ('adventureworks2019', NOINDEX) 
-- 3 seconds - But not RECOMMENDED on Prod Environment
-- Output - Messages - CHECKDB found 0 allocation errors and 0 consistency errors in database 'AdventureWorks2019'.

-- SUPPRESSES ALL INFORMATIONAL MESSAGES (USE IN A LARGE DATABASE)
USE [master]
GO

DBCC CHECKDB WITH NO_INFOMSGS


-- DISPLAYS THE ESTIMATED AMOUNT OF TEMPDB SPACE NEEDED TO RUN DBCC CHECKDB (if you want to unload the integrity check to the tempdb)
DBCC CHECKDB ('TEMPDB') WITH ESTIMATEONLY




-- THIS CHECKS PHYSICAL ON-DISK STRUCTURES, BUT OMITS THE INTERNAL LOGICAL CHECKS
DBCC CHECKDB ('adventureworks2019') WITH PHYSICAL_ONLY




-- 42. USING SQL SERVER AGENT WITH MULTIPLE STEPS
-------------------------------------------------------------------------------------------------------------

-- EXECUTE A FULL DATABASE BACKUP
Use Admin
Go

Backup Database [Admin]
To Disk = N'I:MSSQL\Admin.bak'
WITH NOINIT,
NAME = N'Admin-Full Database Backup'
Go

-- Running the DBCC CHECKDB: Check the logical and physical integrity of all the objects in the specified database 
-- In a production server, where the database may be very large, running this can cause performance issues; WHICH ADVISABLE NOT TO RUN IT CONCURRENTLY WITH OTHER THINGS AND SECONDLY IS ADVISABLE TO RUN IT OFF HOURS, thus run with NO_INFOMSGS which suppresses all informational messages and other options 

Use Admin
Go

DBCC CheckDB

-- Once the output show the message below it means the database is intact there are no issues
-- Output - Messages - CHECKDB found 0 allocation errors and 0 consistency errors in database 'Admin'.


-- STEPS -- Right click on the Jobs - Click New Job - Name Full DB Admin Backup - Enable (if is not enable) - Click Steps - Click New - General - Name Full DB Admin Backup - Type - Transact-SQL script /T-SQL) - Database - master - Command - copy the script inside the box - 
BACKUP Database [Admin]
TO  DISK = N'I:\MSSQL\Admin.bak'
WITH
NOINIT,
NAME = N'Admin-Full Database Backup'
GO 
-- Parse (Parse the command text) - Ok - Click - New again to add second job - Step name - dbcc checkdb - copy and paste the script in to the window box
Use Admin
Go

DBCC CheckDB
-- Check parse - Ok - So now for the JOBS to be view - Go to ADVANCE - Output file - Click the Eclipse btn - Select where you like to save the output and you can also create a folder for it called it - Textfiles - Select the file - File name - dbcc txt - Ok - Now the single job has two steps - First is to BACKUP DATABASE and second is to run DBCC Checkdb - Ok 
-- Now the JOB has been created - Open jobs - Local Jobs - there is the job - Full DB Admin Backup - Right click on it and Click START THE JOB - START.
-- If you open Jobs - Local Jobs - You will see - Full Admin Backup





-- 43. HOW TO SET UP SQL DATABASE MAIL
--------------------------------------------------------------------------------------------------------------------

-- CONFIGURING AND SETTING UP SQL DATABASE MAIL
	-- Database Mail is an enterprise solution for sending e-mail messages from the SQL Server Database Engine. Using Database Mail, your database applications can send e-mail messages to users. Database Mail is not active by default. To use Database Mail, you must explicitly enable Database Mail by using either the Database Mail Configuration Wizard or sp_configure stored procedure. Once it has been enabled, you can configure SQL mail using either GUI (Wizard) or by script.

	-- After the Account and the Profile are created successfully, we need to configure the Database Mail. To configure it, we need to enable the Database Mail XPs parameter through the sp_configure stored procedure, as shown here:

sp_CONFIGURE -- shows the options for server properties (about 17)
GO

-- Re configure in other to see what w are looking for (DATABASE MAIL XPs)
sp_CONFIGURE 'Show Advanced', 1 -- shows the options for server properties (about 69)
GO
RECONFIGURE
GO

-- Now lets look again
sp_CONFIGURE -- shows the options for server properties (about 83 rows)
GO -- There w go (DATABASE MAIL XPs)


-- Now we need to set --> Run_Value to 1 from 0
sp_CONFIGURE 'Database Mail XPs',1 -- configure the database mail and activate it
GO
RECONFIGURE
GO

-- CHECK IT AGAIN
sp_CONFIGURE -- shows the options for server properties and its has activated 
GO


--------------------------------------------------
-- NOTE! => Since will turn all all the options from 17 to 69 for security bridge(reason) w need to turn it OFF back

sp_CONFIGURE 'Show Advanced', 0 -- set the options off for server properties
GO
RECONFIGURE
GO

-- CHECK IT AGAIN 
sp_CONFIGURE -- shows the options for server properties (abouts 17 rows)
GO



--------------------------------------------------------------------------------------------------

-- SETTING UP SQL DATABASE MAIL VIA WIZARD:

-- GUI METHOD => Open Management - Right click on Database Mail - Configure Database Mail - Next - Set up Database Mail by performing the following - 1. Create a new mail, 2. Specify profile security, 3. Configure system parameters - Next - Profile Name - SQL - Click ADD to add the account - Account Name - SQL - Email address - worldwid01@gmail.com - Server name - smtp.gmail.com - Change the port Number - from 25 to 587 - Check the box - Server requires a secure connection (SSL) - Basic authentication - User name - Put the Email Account (worldwid01@gmail.com) - Password - (for that email) - Ok - Next - Set it to - PUBLIC - Next - Next - Finish - Close

-- Go to the SQL Server Agent - Right click - Properties - Alert System - Enable mail profile - Mail profile - SQL - Ok - Now w need to RESTART SQL SERVER AGENT - Right click on SQL Server Agent - Restart - Yes - Verify - Go to your email - Go to Management - Right click on - Database Mail - Send Test E-Mail.. - To - Type in your email - Click Send Test E-mail - Ok - This is a test e-mail sent from Database Mail on ...

-- OPERATORS - Lets create an OPERATOR - Open Management - Right click NEW OPERATOR - Name(Make yourself) - Hammed - E-mail name - worldwid01@gmail.com - Enable - Notifications - Alerts (Enable) - Ok 

-- Open OPERATORS - You will see your Operator Name there - Doublr click it - General - Change from Alerts to Jobs(Select Jobs) - Ok

-- LETS CHECK JOBS NOW 
   -- Jobs - Full Admin Backups - Double click it - Steps - 2 steps - Notification - Select Email - Drop down select your Operator - Select -When the Job succeeds - Ok (but mainly it also good to select FAILED JOB)
   -- RUN the jobs - Right click  - Start Job at Step... - Start. - Check your email for the succeeds email 

	-- DATABASE MAIL CONFIGURATION THROUGH SCRIPTS
	-- When you need to setup Database Mail on dozens of SQL Server instances, rather than perform this tedious task using the SSMS GUI, use the following script that saves me a lot of time. Below is the template. The sysmail_add_account_sp @username and @password parameters might be required depending on your SMTP server authentication and you will of course need to customize the mail server name and addresses for your environment.

/**

-- SCRIPT TO SEE ALL THE DATABASE MAIL ACCOUNT
EXEC msdb.dbo.sysmail_help_account_sp;

-- SCRIPT TO SEE ALL THE DATABASE MAIL ACCOUNT
EXEC msdb.dbo.sysmail_help_profileaccount_sp;


-- SCRIPT TO DELETE DATABASE MAIL PROFILE ACCOUNT
EXECUTE msdb.dbo.sysmail_delete_profile_sp  
    @profile_name = 'My_Profile' ;  

-- SCRIPT TO DELETE DATABASE MAIL ACCOUNT
EXECUTE msdb.dbo.sysmail_delete_account_sp
    @account_name = 'SQL'; -- Change the account name here


-- SCRIPT TO START DATABASE MAIL
EXEC msdb.dbo.sysmail_start_sp



-- SCRIPT TO SEND EMAIL FROM DATABASE MAIL FOR TESTING
Execute msdb.dbo.sp_send_dbmail
@recipients = 'worldwid01@gmail.com',
@body = 'TestSQL2019',
@SUBJECT = 'TestSQL2019'


-- Database Mail is enabled SCRIPT to start Database Mail External Programm
Exec msdb.dbo.sysmail_start_sp;

-- SCRIPT TO STOP OR DISABLE DATABASE MAIL
EXEC msdb.dbo.sysmail_stop_sp

-- To confirm that Database Mail External Programm is started
Exec msdb.dbo.sysmail_help_status_sp


Exec msdb.dbo.sysmail_help_queue_sp @queue_type = 'mail'

-- SCRIPT TO CONFIGURE DATABASE MAIL 
DECLARE @YourEmail NVARCHAR(50) SET @YourEmail = 'worldwid@gmail.com' --Put here your E-mail
DECLARE @YourPassword NVARCHAR(50) SET @YourPassword = 'bnllvzsfmxkecmdw' --Put here your Password
DECLARE @YourSMTPSserver NVARCHAR(50) SET @YourSMTPSserver = 'SMTP.gmail.com' --Put here your SMTP Server
DECLARE @YourPort int SET @YourPort = '587' --Put here your SMTP port

-- Create a Database Mail account  
EXECUTE msdb.dbo.sysmail_add_account_sp  
    @account_name = 'My_Account',  
    @description = 'Mail account for you.',  
    @email_address = @YourEmail,  
    @replyto_address = @YourEmail,
	@display_name = 'Your Mailer Account',  
    @mailserver_name = @YourSMTPSserver, 
	@port = @YourPort,
	@use_default_credentials = 0,
	@username = @YourEmail,
	@password = @YourPassword;


-- Create a Database Mail profile  
EXECUTE msdb.dbo.sysmail_add_profile_sp  
    @profile_name = 'My_Profile',  
    @description = 'Your profile used for your e-mail' ; 

-- Add the account to the profile  
EXECUTE msdb.dbo.sysmail_add_profileaccount_sp  
    @profile_name = 'My_Profile',  
    @account_name = 'My_Account',  
    @sequence_number =1 ; 

-- Grant access to the profile to the DBMailUsers role  
EXECUTE msdb.dbo.sysmail_add_principalprofile_sp  
    @profile_name = 'My_Profile',  
    @principal_name = 'public',  
    @is_default = 0; 

EXEC msdb.dbo.sysmail_help_account_sp;

-- show advanced options
EXEC sp_configure 'show advanced options', 1
GO
RECONFIGURE
GO
 
-- enable Database Mail XPs
EXEC sp_configure 'Database Mail XPs', 1
GO
RECONFIGURE
GO
 
-- check if it has been changed
EXEC sp_configure 'Database Mail XPs'
GO
 
-- hide advanced options
EXEC sp_configure 'show advanced options', 0
GO
RECONFIGURE
GO

*/



--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-- 1. ENABLE DATABASE MAIL FOR THIS INSTANCE
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
EXECUTE sp_configure 'show advanced', 1;
RECONFIGURE;
EXECUTE sp_configure 'Database Mail XPs',1;
RECONFIGURE;
GO

-- 2. CREATE A DATABASE MAIL ACCOUNT
EXECUTE msdb.dbo.sysmail_add_account_sp
@account_name = 'SQL', -- SQL for example
@description = 'Account used by all mail profiles.',
@email_address = 'dbareport01@gmail.com', -- enter your email address here
-- @replyto_address = 'dbareport01@gmail.com', -- enter your email address here (note! Reply has been commited out as I had issues with setting msn account)
@display_name = 'Database Mail',
@mailserver_name = 'smtp.gmail.com'; -- enter your server name here for example smtp.live.com for msn.com


--3. CREATE A DATABASE MAIL PROFILE
EXECUTE 
msdb.dbo.sysmail_add_profile_sp
@profile_name = 'Default Public Profile',
@description = 'Default public profile for all users';


-- 4.ADD THE ACCOUNT TO THE PROFILE
EXECUTE msdb.dbo.sysmail_add_profileaccount_sp
@profile_name = 'Default Public Profile',
@account_name = 'SQL', -- SQL
@sequence_number = 1;


-- 5.GRANT ACCESS TO THE PROFILE TO ALL MSDB DATABASE USERS
EXECUTE msdb.dbo.sysmail_add_principalprofile_sp
@profile_name = 'Default Public Profile',
@principal_name = 'public',
@is_default = 1;
GO

-- 6.SEND A TEST EMAIL
EXECUTE msdb.dbo.sp_send_dbmail
@subject = 'Test Database Mail Message',
@recipients = 'dbareport01@gmail.com', -- enter your email address here
@query = 'SELECT @@VERSION'; -- Gives you the version of SQL 
GO



-- View information about mail in msdb database using the following scripts:
-- Database Mail keeps copies of outgoing e-mail messages and other information about mail and displays them in msdb database using the following scripts:
use msdb
go

SELECT * FROM sysmail_server

SELECT * FROM sysmail_allitems

SELECT * FROM sysmail_sentitems

SELECT * FROM sysmail_unsentitems

SELECT * FROM sysmail_faileditems

SELECT * FROM sysmail_mailitems

SELECT * FROM sysmail_log


-- GUI Method -- To see all those information by GUI 
------------------------------------------------------
-- Open - Management - Right click on Database Mail - Click Vie Database Mail Log 

-- 44. SETTING UP SQL DATABASE MAIL Part 2 -- EXPLANATION ON PART 1 ABOVE

-- https://www.youtube.com/watch?v=WM1HThxOu4I
------------------------------------------------------------------------
-- Password: nnprjiufudcbojsd

-- Gmail Account: dbareport01@gmail.com


-- sp_configure Change 

sp_configure 'show advanced options', 1;
GO
RECONFIGURE;
GO
 
sp_configure 'Database Mail XPs', 1;
GO
RECONFIGURE
GO


-- Gmail SMTP Server 
-- Server Name : smtp.gmail.com
-- Port: 587
-- SSL connection: Enabled





-- 45. SETTING UP ALERTS SEVERITY ERRORS FROM 17 to 25 
------------------------------------------------------------------------

-- ALERTS: SEVERITY LEVELS

-- •	What are alerts?
-- •	Setting up 17 thought 25 alerts via script
-- •	Mapping alerts to DBA ADMIN operator
-- •	Example of creating an alert for T-LOG Full

-- Events are generated by SQL Server and entered into the Microsoft Windows application log. SQL Server Agent reads the application log and compares events written there to the alerts that you have defined or created. When SQL Server Agent finds a match, it fires an alert

-- TYPES OF PROBLEM: 
-- From 1 - 25 But 1 to 16 are not serious errors alerts
   -- Severity level 10 messages are informational -- !DO NOT NEED TO SET UP ALERTS FOR THEM
   -- Severity levels from 11 through 16 are generated by the user
   -- Severity levels from 17 through 25 indicate software or hardware errors
   -- When a level 17, 18, or 19 errors occurs, you can continue working but you should check the error log 

-- If the problem affects an entire database, you can use DBCC CHECKDB (database) to determine the extent of the damage


-- SEVERITY LEVEL 17: INSUFFICIENT RESOURCES
   -- These messages indicate that the statement caused SQL Server to run out of resources (such as locks or disk space for the database) 


-- SEVERITY LEVEL 18: NONFATAL INTERNAL ERROR DETECTED
   -- These messages indicate that there is some type of internal software problem, but the statement finishes, and the connection to SQL Server is maintained. For example, a severity level 18 message occurs when the SQL Server query processor detects an internal error during query optimization. 


-- SEVERITY LEVEL 19: SQL SERVER ERROR IN RESOURCE
   -- These messages indicate that some no configurable internal limit has been exceeded and the current batch process is terminated. Severity level 19 errors occur rarely


-- SEVERITY LEVEL 20: SQL SERVER FATAL ERROR IN CURRENT PROCESS
   -- These messages indicate that a statement has encountered a problem. Because the problem has affected only the current process, it is unlikely that the database itself has been damaged.


-- SEVERITY LEVEL 21: SQL SERVER FATAL ERROR IN DATABASE (DBID) PROCESSES
   -- These messages indicate that you have encountered a problem that affects all processes in the current database; however, it is unlikely that the database itself has been damaged.


-- SEVERITY LEVEL 22: SQL SERVER FATAL ERROR TABLE INTEGRITY SUSPECT
   -- These messages indicate that the table or index specified in the message has been damaged by a software or hardware problem.


-- SEVERITY LEVEL 23: SQL SERVER FATAL ERROR: DATABASE INTEGRITY SUSPECT
   --These messages indicate that the integrity of the entire database is in question because of a hardware or software problem.


-- SEVERITY LEVEL 23 ERRORS OCCUR RARELY; DBCC CHECKDB TO DETERMINE THE EXTENT OF THE DAMAGE MAY BE NECESSARY TO RESTORE THE DATABASE.


-- SEVERITY LEVEL 24: HARDWARE ERROR
   -- These messages indicate some type of media failure. The system administrator might have to reload the database. It might also be necessary to call your hardware vendor.


-- GUI METHODS OF SQL SERVER AGENT ALERTS -- Click SQL Server Agent - Alerts (you can refresh it) - 


use master
go 

Create Database Alerts
go

DBCC sqlperf (logspace) -- To see the space the database (Alerts) is using 
-- Database Name  Log Size(MB)     Log Space Used %     Status
-- Alerts	      7.992188	       4.991446	            0


-- GUI SETTING UP SQL SERVER AGENT ALERTS
-- Example of creating an alert using the GUI

-- Now let SET UP ALERTS IF THE T.LOG UTILIZES 500KB OF ABOVE LOG SPACE USED THEN I WANT ALERTS TRIGERED IMMEDIATELY AND SEND TO ME BY EMAIL THAT THIS T.LOG is getting full to what I defined

-- TO DO THAT BY GUI
-- General
   -- Click SQL Server Agent - Right click on Alerts - New Alerts 
   -- Name: T log is getting full 
   -- Enable: Tick the box
   -- Type: SQL Server performance condition alert
   -- Object: Databases
   -- Counter: Log File(s) Used Size (KB)
   -- Instance: Alerts (Database will created above)
   -- Alert if counter: 500 (pressure) if the rises above 500 KB
   -- Value: 0

-- Response
   -- Notify operators -- Select Notify operators
      -- Operator List: Email -- Select the email you want to be getting the alert

-- Option
   -- Email: Select the box

-- Ok

-- Now you check the ALERTS under the SQL Server Agent the Alerts you created will be there
use Alerts 
go

Create table WHILE_TABLE
(FIRSTNAME varchar(8000))
go

-- IMPORT DATA FROM ADVENTUREWORKS
-- Right click on the database name ADVENTUREWORKS2019 - Task - Export Data - Next - Data source: SQL Server Native Client 11.0 - Server name: SQL01 - Database name: ADVENTUREWORKS2019 - - Next - Destination - SQL Server Native Client 11.0 - Server name: . (use dot it work for all servername which local) - Database: Alerts - Next - Next - Choose one or more tables and views to copy: Select: Sales.SalesOrderDetails - Next - Check: Run immediately - Next - Finish



-- SETTING UP SQL SERVER AGENT ALERTS
USE [msdb]
GO

-- SEVERITY LEVEL 17: INSUFFICIENT RESOURCES
EXEC msdb.dbo.sp_add_alert @name=N'Error 17 Alert', 
  @message_id=0, 
  @severity=17, 
  @enabled=1, 
  @delay_between_responses=0, 
  @include_event_description_in=1;
GO

-- SEVERITY LEVEL 18: NONFATAL INTERNAL ERROR DETECTED
EXEC msdb.dbo.sp_add_alert @name=N'Error 18 Alert', 
  @message_id=0, 
  @severity=18, 
  @enabled=1, 
  @delay_between_responses=0, 
  @include_event_description_in=1;
GO

-- SEVERITY LEVEL 19: SQL SERVER ERROR IN RESOURCE
EXEC msdb.dbo.sp_add_alert @name=N'Error 19 Alert', 
  @message_id=0, 
  @severity=19, 
  @enabled=1, 
  @delay_between_responses=0, 
  @include_event_description_in=1;
GO

-- SEVERITY LEVEL 20: SQL SERVER FATAL ERROR IN CURRENT PROCESS
EXEC msdb.dbo.sp_add_alert @name=N'Error 20 Alert', 
  @message_id=0, 
  @severity=20, 
  @enabled=1, 
  @delay_between_responses=0, 
  @include_event_description_in=1;
GO

-- SEVERITY LEVEL 21: SQL SERVER FATAL ERROR IN DATABASE (DBID) PROCESSES
EXEC msdb.dbo.sp_add_alert @name=N'Error 21 Alert', 
  @message_id=0, 
  @severity=21, 
  @enabled=1, 
  @delay_between_responses=0, 
  @include_event_description_in=1;
GO

-- SEVERITY LEVEL 22: SQL SERVER FATAL ERROR TABLE INTEGRITY SUSPECT
EXEC msdb.dbo.sp_add_alert @name=N'Error 22 Alert', 
  @message_id=0, 
  @severity=22, 
  @enabled=1, 
  @delay_between_responses=0, 
  @include_event_description_in=1;
GO

-- SEVERITY LEVEL 23: SQL SERVER FATAL ERROR: DATABASE INTEGRITY SUSPECT
-- SEVERITY LEVEL 23 ERRORS OCCUR RARELY; DBCC CHECKDB TO DETERMINE THE EXTENT OF THE DAMAGE MAY BE NECESSARY TO RESTORE THE DATABASE.
EXEC msdb.dbo.sp_add_alert @name=N'Error 23 Alert', 
  @message_id=0, 
  @severity=23, 
  @enabled=1, 
  @delay_between_responses=0, 
  @include_event_description_in=1;
GO


-- SEVERITY LEVEL 24: HARDWARE ERROR
EXEC msdb.dbo.sp_add_alert @name=N'Error 24 Alert', 
  @message_id=0, 
  @severity=24, 
  @enabled=1, 
  @delay_between_responses=0, 
  @include_event_description_in=1;
GO

EXEC msdb.dbo.sp_add_alert @name=N'Error 25 Alert', 
  @message_id=0, 
  @severity=25, 
  @enabled=1, 
  @delay_between_responses=0, 
  @include_event_description_in=1;
GO


-- SCRIPT TO CONFIGURING THE SQL SERVER OPERATOR
USE msdb;
GO

-- CREATE OPERATOR DBA_ADMIN
EXEC msdb.dbo.sp_add_operator
  @name = 'DBAs', -- DBA_ADMIN or dbareport01
  @enabled = 1,
  @email_address = 'DBAs@mycompany.com'; -- Replace with your email address for the operator -- dbareport01@gmail.com
GO 

EXEC msdb.dbo.sp_add_notification 
  @alert_name = N'Error 17 Alert',
  @operator_name = 'DBAs',
  @notification_method = 1;
GO

EXEC msdb.dbo.sp_add_notification 
  @alert_name = N'Error 18 Alert',
  @operator_name = 'DBAs',
  @notification_method = 1;
GO

EXEC msdb.dbo.sp_add_notification
  @alert_name = N'Error 19 Alert',
  @operator_name = 'DBAs',
  @notification_method = 1;
GO

EXEC msdb.dbo.sp_add_notification 
  @alert_name = N'Error 20 Alert',
  @operator_name = 'DBAs',
  @notification_method = 1;
GO

EXEC msdb.dbo.sp_add_notification 
  @alert_name = N'Error 21 Alert',
  @operator_name = 'DBAs',
  @notification_method = 1;
GO

EXEC msdb.dbo.sp_add_notification 
  @alert_name = N'Error 22 Alert',
  @operator_name = 'DBAs',
  @notification_method = 1;
GO

EXEC msdb.dbo.sp_add_notification 
  @alert_name = N'Error 23 Alert',
  @operator_name = 'DBAs',
  @notification_method = 1;
GO

EXEC msdb.dbo.sp_add_notification 
  @alert_name = N'Error 24 Alert',
  @operator_name = 'DBAs',
  @notification_method = 1;
GO

EXEC msdb.dbo.sp_add_notification 
  @alert_name = N'Error 25 Alert',
  @operator_name = 'DBAs',
  @notification_method = 1;
GO


-- CENTRAL MANAGEMENT WITH SQL SERVER AGENT
-- WHAT IS MULTI SERVER ADMINISTRATION?
   -- When you have multiple SQL Servers that need the exact same SQL Server Agent Job created or maintenance plans, then rather than creating scripts and running on each server, configure and manage your multiple server via Central Server Administration option

-- This option is very useful when you need to create and run the same jobs or maintenance plans across many SQL Server instances

-- STEPS TO SET UP CENTRAL SERVER MANAGEMENT:
  -- •	Register all servers in SQL Server Management Studio (SSMS)
  -- •	Create Server Groups if necessary for collective administration
  -- •	Select Multi Server administration on SQL Agent and select which Server should be Master and which should be Targets and follow the wizard

-- (If this error (The enlist operation failed (reason: SQLServerAgent Error: The target server cannot establish an encrypted connection to the master server 'Server Name'. Make sure that the MsxEncryptChannelOptions registry subkey is set correctly on the target server pops ups, then we have to go to the ‘regedit’ and configure the registry and change the 'HKLM\SOFTWARE\Microsoft\Microsoft SQL Server\\SQLServerAgent and change the MsxEncryptChannelOptions' value to '0'. Save the change and close the Registry editor 

-- To set up a Multi Server Job under SQL Server Agent you must connect to the master server in SSMS and navigate to 'SQL Server Agent' | 'Jobs' | 'Multi-Server Jobs". Configure the job as needed and on the 'Targets tab' select the target servers you want this job to execute on.


-- STEP BY STEP OF CREATING CENTRAL SERVER
-- Do this step 1 for all the instance or server i.e SQL01,SQL02 and SQL03
-- 1. Run - Regedit - Ok - Software - Microsoft - Microsoft SQL Server - MSSQL15.MSSQLSERVER(which is server instance 1) - SQL SERVER Agent - Double click - MsxEncryptChannelOptions - Change the value from 2 to 0 - Click Ok

-- Now let registered the servers 
-- 2. Click View (at the top of ssms) - Open Database Engine - Right click: Local Server Groups - New Server Registration - General - Server name - SQL01 (select the first server) - Test - Ok - Save
-- 3. View (at the top of ssms) - View (at the top of ssms) -Right click - Central Management Servers - Register Central Management Server - General - Server name - SQL01 (select the first server) - Save
-- 4. View (at the top of ssms) - Under the Central Management Servers - Right click on the first server - SQL01 - Click New Server Registration - Select server 2 - SQL02 - Save
-- 5. View (at the top of ssms) - Under the Central Management Servers - Right click on the first server - SQL01 - Click New Server Registration - Select server 3 - SQL03 - Save
-- 6. Close the Registered Servers will are done here

-- 7. On Server 1 - SQL01 - Right click on SQL Server Agent - Multi Server Administration - Make this Master... - Next - Provide the Operator i.e the Email - dbareport01@gmail.com - Next - Open the Database Engine - Open the - Central Management Servers - Open the Server 1(SQL01) - Now we need to select which one is the Taarget - Select Server 2 - SQL02 - Click Push sign (>) to push it to the right - After select Server 3 SQL03 - Push it to the right (>) - You will see the SQL02 and SQL03 at the Target Servers Box: - Next - Close the pop up windwo - Successful - Click the BOX if its not checked - Create a new login if necessary and assign it rights to the MSX. - Next - Finish - Success.
-- 8. Now you will see on the first Server - SQL01 - SQL Server Agent(MSX) and if you open SQL Server Agent - Jobs - You will see - Multi Server Jobs
-- 9. Now you will see on the second Server - SQL02 - SQL Server Agent(TSX: SQL01)
-- 10. Now you will see on the second Server - SQL03 - SQL Server Agent(TSX: SQL01)

-- Good work

-- Now lets create a Jobs - On Server one - SQL01 - Right click on the - Multi Server Jobs - New Job... - General - Name: - New DB - Steps: New - Step name: - New DB - Database: Master - Command: Create Database NEW - Go - Ok - Targets: Select: Target multiples servers - Now select which server you want it to have newly created database - You can select both of them or any one you want it to have it - Check the box for SQL02 and SQL03. - Ok

-- Now if you check under the Multi Server Jobs - You will see New DB
-- If you check SQL02  - Databases - There would not be any New database 
-- But once will EXCUTE the job from the SQL01  - SQL Server Agent(MSX) - Jobs - Multi Server Jobs - Right click on: New DB: Start the job step... - it will excute the jobs - Successful - Now the Database will be created 
-- The benefit is to save you time if you have many many server and you want to create Database for all it really help




-- 48. INSTALLING FREE VMWARE PRO PLAYER FOR VIRTUALIZATION
-- https://www.vmware.com/products/player

-- TRY FOR FREE - DOWNLOAD - After download  - Double click on the download file - Next - Next - Next - Next - Finish - VMware Workstation 17.5 Player - Installed
-- Open: VMware Workstation 17.5 Player - Create a New Virtual Machine - 
-- 




-- 49. INTRODUCTION TO SQL SERVER SECURITY
--------------------------------------------------------------------------------------------------------------------------------------------------
-- UNDERSTANDING WINDOWS AND SQL SECURITY USERS
   -- •	Windows username  -- Account that accesses a Windows Domain
   -- •	Windows Groups  -- Account that accesses a Windows Domain via a group
   -- •	SQL User  -- Account that accesses SQL Server Databases 
   -- •	SQL Login  -- Account that accesses SQL Server 
   -- •	SQL Roles  -- Account that accesses SQL Server Roles
   -- •	What is Windows username and mapping? – Refers to associating Windows username to SQL Logins


-- 50. DEMONSTRATION OF USERS LOGINS ROLES
--------------------------------------------------------------------------------------------------------------------------------------------------
   -- SQL User
      -- From AD(SQLAD) - Tools - Active Directory Users and Computers - Users - Right click on User - New User - Name: Tom - Logon Name - Tom - Next - Password - Repeat Password - Password Never expire - Next - Finish 
   
   -- SQL Login
      -- From SQL01 - Security - Logins - Right click on Logins - New Login - Login Name - Search: - Provide the User name you created at the AD(SQLAD) - Location: Entire Directory - Chech Name - Ok - Mapping: To give access to the user for any database - or Just click Ok.
	                                                                                                                                                                                      -- Databases: To give access to the user on any of the Database and permission to which column or which table the user have access - Open the Database - Security - User -  Right click on the User - New user - Here you can decide kind of permission to give the user
																																														  

-- 51. MANAGING SECURITY WITH T-SQL
--------------------------------------------------------------------------------------------------------------------------------------------------

-- CREATING A WINDOWS USER FOR SQL SERVER 

USE [master]
GO

CREATE LOGIN [SQL\tom] --<< Create a SQL Login of Windows user account to access the SQL Server ONLY
FROM WINDOWS 
WITH DEFAULT_DATABASE=[master], 
DEFAULT_LANGUAGE=[us_english]
GO


USE [AdventureWorks2012]
GO
 
CREATE USER [Tom] --<< Create a SQL User using SQL Login to access the database
FOR LOGIN [SQL\tom] --<< SQL Login
WITH DEFAULT_SCHEMA=[dbo]
GO


use [AdventureWorks2012]
GO

GRANT SELECT --<< Grant permission to SQL User to access a table
ON [HumanResources].[Department] 
TO [Tom] AS [dbo]
GO


-- DROP THE SQL LOGIN DOES NOT DROP THE SQL USER. IT MUST BE DROP SEPARATELY
---------------------------------------------------------------------------------------------------------------------------------------
Use master
Go

DROP LOGIN [SQL\tom] --<< DROP SQL LOGIN
Go


Use [AdventureWorks2019]
Go

DROP USER [Tom] --<< DROP SQL USER 
Go



-- 52. SELECTING AUTHENTICATION OR MIXED MODE SECURITY
--------------------------------------------------------------------------------------------------------------------------------------------------


-- CHOOSING AUTHENTICATION FOR SQL SERVER
   -- During the installation of SQL Server, an option for security is asked for, that is, the type of authentication you want SQL to apply; either Windows Authentication mode and mixed mode

 
-- If you chose Windows Authentication mode, then Windows Authentication will disables SQL Server Authentication by default

-- If you chose mixed mode (Windows Authentication and SQL Server Authentication) then you must provide and then confirm a strong password for the built-in SQL Server system administrator account named sa. 

-- Because the sa account is well known and often targeted by malicious users, do not enable the sa account unless your application requires it and even then be very careful in giving it out. I create another account and give it sysadmin rights if need be.  Ideally, you should not activate the sa account and if you do, rename the sa account.


-- VERIFYING CONNECTION VIA WINDOWS AUTHENTICATION
--------------------------------------------------------------------
   -- When a user connects through a Windows user account, SQL Server validates the account name and password using the Windows principal token in the operating system. As Windows authentication has built in security protocol, password policy enforcement, complexity for strong passwords, support for account lockout, and supports password expiration it’s recommended over SQL Authentication


-- VERIFYING CONNECTION VIA SQL AUTHENTICATION
--------------------------------------------------------------------
   -- When using SQL Server Authentication, logins are created in SQL Server that is not based on Windows user accounts. Both the user name and the password are created by using SQL Server and stored in SQL Server. 

-- SOME REASONS TO USE SQL SERVER AUTHENTICATION
--------------------------------------------------------------------
   -- •	Support older applications 
   -- •	Third parties applications that require SQL Server Authentication
   -- •	Supports mixed operating systems, where all users are not authenticated by a Windows domain
   -- •	Support Web-based applications where users create their own identities
   -- •	Allows software developers to distribute their applications by using a complex permission hierarchy 






-- 53. ACCESSING SQL SERVER USING SQL LOGIN
--------------------------------------------------------------------------------------------------------------------------------------------------

--CREATE A SQL LOGIN NOT USING WINDOWS USER

USE [master]
GO

CREATE LOGIN [Mary] 
WITH PASSWORD=N'1234@Admin' --<< password does not meet complexity (use upper 'P')
MUST_CHANGE, DEFAULT_DATABASE=[master], 
CHECK_EXPIRATION=ON, 
CHECK_POLICY=ON
GO

USE [AdventureWorks2019]
GO

CREATE USER [Mary] FOR LOGIN [Mary] --<< Create a SQL Login
GO

USE [AdventureWorks2019]
GO

ALTER ROLE [db_datareader] --<< Add SQL Login to database Role (db_datareader)
ADD MEMBER [Mary]
GO



-- GUI - SQL01 (SQL Server) - Security - Login - New Login: - Login name: Mary - 
